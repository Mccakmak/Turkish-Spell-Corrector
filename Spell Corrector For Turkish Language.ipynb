{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Corrector For Turkish Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from os import listdir                                         # Directory list\n",
    "import pandas as pd                                            # To see clear description\n",
    "from sklearn.model_selection import train_test_split           # To split the data into test, training\n",
    "import numpy as np                                             # Random number\n",
    "from nltk import tokenize                                      # Sentence tokenizer\n",
    "import re                                                      # Remove punctuation\n",
    "import copy                                                    # Deep copy\n",
    "import tensorflow as tf                                        # Model\n",
    "import time                                                    # Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorflow version\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aklından Bir Sayı Tut - John Verdon.txt',\n",
       " 'Anna Karenina - Lev Nikolayeviç Tolstoy.txt',\n",
       " 'Aşk ve Gurur - Jane Austen.txt',\n",
       " 'Beyaz Diş - Jack London.txt',\n",
       " 'Beyaz Zambaklar Ülkesinde.txt',\n",
       " 'Bülbülü Öldürmek - Harper Lee.txt',\n",
       " \"Erkekler Mars'tan, kadınlar Venüs'ten.txt\",\n",
       " 'Eylül - Mehmed Rauf.txt',\n",
       " 'Gurur ve Önyargı - Jane Austen.txt',\n",
       " 'Harry Potter ve Felsefe Taşı - J. K. Rowling.txt',\n",
       " 'Hatasız Düşünme Sanatı - Rolf Dobelli.txt',\n",
       " 'Karamazov Kardeşler - Fyodor Mihailoviç Dostoyevski.txt',\n",
       " 'Körlük - José Saramago.txt',\n",
       " 'Peygamberler Tarihi - Asim Koksal.txt',\n",
       " 'Rothschild Para İmparatorluğu - George Armstrong.txt',\n",
       " 'Saatleri Ayarlama Enstitüsü - Ahmet Hamdi Tanpınar.txt',\n",
       " 'Satranç - Stefan Zweig.txt',\n",
       " \"Sherlock Holmes'ün Maceraları.txt\",\n",
       " 'Simyacı - Paulo Coelho.txt',\n",
       " 'Sineklerin Tanrısı - William Golding.txt',\n",
       " 'Suç ve Ceza - Fyodor Mihailoviç Dostoyevski.txt',\n",
       " 'Taht Oyunları - George R.R. Martin.txt',\n",
       " 'Tesla - Anlaşılamamış Dahi - Margaret Cheney.txt',\n",
       " 'Yeraltından Notlar - Fyodor Mihailoviç Dostoyevski.txt',\n",
       " 'Yüzüklerin Efendisi - J.R.R. Tolkien.txt',\n",
       " 'İnsan Ne İle Yaşar - Lev Nikolayeviç Tolstoy.txt',\n",
       " 'İçimizdeki Şeytan - Sabahattin-Ali.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Book File Names\n",
    "path = 'books/'\n",
    "files = listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"İÇİNDEKİLER\\n\\n\\n\\nTanıtım\\n\\n\\n\\nÖvgüler\\n\\n\\n\\nGiriş\\n\\n\\n\\nBirinci Kısım\\n\\nÖlümcül Anılar\\n\\n\\n\\nPolisin sanatı\\n\\nKusursuz kurban\\n\\n\\n\\nCennette bela\\n\\n\\n\\nSeni o kadar iyi tanıyorum ki, ne düşündüğünü biliyorum\\n\\n\\n\\nKötü olasılıklar\\n\\n\\n\\nBoyanmış gül kadar kırmızı kan için\\n\\n\\n\\nKara delik\\n\\n\\n\\nKaya ve taş\\n\\n\\n\\nEşsiz adam\\n\\n\\n\\nKusursuz mekan\\n\\n\\n\\nEşsiz hizmet\\n\\n\\n\\nDürüstlüğün önemi\\n\\nSuçlu hissetmek gereksiz\\n\\n\\n\\nMeydan okuma\\n\\n\\n\\nÇatışma\\n\\n\\n\\nBaşlangıcın sonu\\n\\n\\n\\nİkinci Kısım\\n\\n\\n\\nKorkunç Oyunlar\\n\\n\\n\\nKan gölü\\n\\n\\n\\nHiçbir yere gitmeyen ayak izleri\\n\\n\\n\\nDünyanın pislikleri\\n\\n\\n\\nBir aile dostu\\n\\n\\n\\nÖncelikler\\n\\n\\n\\nDüzeltme\\n\\n\\n\\nİz bırakmadan\\n\\n\\n\\nYılın cinayeti\\n\\n\\n\\nGurney'in sorgulanması\\n\\n\\n\\nBoş çek\\n\\n\\n\\nSheridan'ı tanımak\\n\\n\\n\\nOlay yerine dönüş\\n\\n\\n\\nGeriye doğru\\n\\n\\n\\nBüyük köşk\\n\\n\\n\\nBronx'tan gelen telefon\\n\\n\\n\\nÜçüncü Kısım\\n\\n\\n\\nBaşa Dönüş\\n\\n\\n\\nTemizlikçi geliyor\\n\\n\\n\\nBerbat\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read book content\n",
    "books = []\n",
    "for file in files:\n",
    "    with open(path + file, encoding=\"utf8\") as book:\n",
    "        books.append(book.read())\n",
    "books[0][:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93781 words  ==>  Aklından Bir Sayı Tut - John Verdon.txt\n",
      "254903 words  ==>  Anna Karenina - Lev Nikolayeviç Tolstoy.txt\n",
      "90445 words  ==>  Aşk ve Gurur - Jane Austen.txt\n",
      "55548 words  ==>  Beyaz Diş - Jack London.txt\n",
      "30685 words  ==>  Beyaz Zambaklar Ülkesinde.txt\n",
      "59758 words  ==>  Bülbülü Öldürmek - Harper Lee.txt\n",
      "62934 words  ==>  Erkekler Mars'tan, kadınlar Venüs'ten.txt\n",
      "67901 words  ==>  Eylül - Mehmed Rauf.txt\n",
      "82955 words  ==>  Gurur ve Önyargı - Jane Austen.txt\n",
      "55657 words  ==>  Harry Potter ve Felsefe Taşı - J. K. Rowling.txt\n",
      "35176 words  ==>  Hatasız Düşünme Sanatı - Rolf Dobelli.txt\n",
      "236642 words  ==>  Karamazov Kardeşler - Fyodor Mihailoviç Dostoyevski.txt\n",
      "85703 words  ==>  Körlük - José Saramago.txt\n",
      "123858 words  ==>  Peygamberler Tarihi - Asim Koksal.txt\n",
      "35892 words  ==>  Rothschild Para İmparatorluğu - George Armstrong.txt\n",
      "93209 words  ==>  Saatleri Ayarlama Enstitüsü - Ahmet Hamdi Tanpınar.txt\n",
      "17522 words  ==>  Satranç - Stefan Zweig.txt\n",
      "464023 words  ==>  Sherlock Holmes'ün Maceraları.txt\n",
      "28208 words  ==>  Simyacı - Paulo Coelho.txt\n",
      "51892 words  ==>  Sineklerin Tanrısı - William Golding.txt\n",
      "167555 words  ==>  Suç ve Ceza - Fyodor Mihailoviç Dostoyevski.txt\n",
      "220964 words  ==>  Taht Oyunları - George R.R. Martin.txt\n",
      "62263 words  ==>  Tesla - Anlaşılamamış Dahi - Margaret Cheney.txt\n",
      "29641 words  ==>  Yeraltından Notlar - Fyodor Mihailoviç Dostoyevski.txt\n",
      "372271 words  ==>  Yüzüklerin Efendisi - J.R.R. Tolkien.txt\n",
      "18301 words  ==>  İnsan Ne İle Yaşar - Lev Nikolayeviç Tolstoy.txt\n",
      "66947 words  ==>  İçimizdeki Şeytan - Sabahattin-Ali.txt\n"
     ]
    }
   ],
   "source": [
    "# Word count for each book\n",
    "for book_no,book in enumerate(books):\n",
    "    words = book.split()\n",
    "    print(str(len(words)) + \" words  ==>  \" + files[book_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"İÇİNDEKİLER Tanıtım Övgüler Giriş Birinci Kısım Ölümcül Anılar Polisin sanatı Kusursuz kurban Cennette bela Seni o kadar iyi tanıyorum ki, ne düşündüğünü biliyorum Kötü olasılıklar Boyanmış gül kadar kırmızı kan için Kara delik Kaya ve taş Eşsiz adam Kusursuz mekan Eşsiz hizmet Dürüstlüğün önemi Suçlu hissetmek gereksiz Meydan okuma Çatışma Başlangıcın sonu İkinci Kısım Korkunç Oyunlar Kan gölü Hiçbir yere gitmeyen ayak izleri Dünyanın pislikleri Bir aile dostu Öncelikler Düzeltme İz bırakmadan Yılın cinayeti Gurney'in sorgulanması Boş çek Sheridan'ı tanımak Olay yerine dönüş Geriye doğru Büyük köşk Bronx'tan gelen telefon Üçüncü Kısım Başa Dönüş Temizlikçi geliyor Berbat bir gece Karanlık bir gün Işığa doğru sendeleyiş Bir olaydan diğerine Üçlü felaket Zor adam Seninle bir randevumuz var,\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To have appropriate tokenization\n",
    "for book_no, book in enumerate(books):\n",
    "    # Remove extra space\n",
    "    book = \" \".join(book.split())\n",
    "    # ... -> .\n",
    "    book = re.sub(r'\\.\\.\\.','.',book)\n",
    "    \n",
    "    book = re.sub(r'.”', '.', book)\n",
    "    books[book_no] = book\n",
    "    \n",
    "books[0][:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 308954 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Split text to sentences.\n",
    "sentences = []\n",
    "for book in books:\n",
    "    # Tokenize\n",
    "    tokenized_book = tokenize.sent_tokenize(book)\n",
    "    for sentence in tokenized_book:\n",
    "        sentences.append(sentence)\n",
    "print(\"Total {} sentences.\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gözler her zaman işin en zor kısmıydı - gözler ve ağız - ama anahtar noktalardı.',\n",
       " 'Bazen küçücük bir noktanın duruşu ve yoğunluğu üzerinde saatlerce çalışırdı.',\n",
       " 'O kadar uğraşmasına rağmen bazen çok iyi sonuçlar elde edemezdi.',\n",
       " \"Yeterince iyi olmadıkları için, bu sonuçlardan Sonya'ya ve tabii ki Madeleine'e bahsetmezdi.\",\n",
       " 'Gözlerin sırrı, gerilimi ve çelişkiyi her şeyden daha iyi yakalayabilmesindeydi.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[70:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sentences\n",
    "cleaned_sentences = []\n",
    "for sentence in sentences:\n",
    "    # Remove all, hold space and string\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
    "    sentence = re.sub(r'_','',sentence)\n",
    "    # Remove extra space\n",
    "    sentence = \" \".join(sentence.split())\n",
    "    cleaned_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gözler her zaman işin en zor kısmıydı gözler ve ağız ama anahtar noktalardı',\n",
       " 'Bazen küçücük bir noktanın duruşu ve yoğunluğu üzerinde saatlerce çalışırdı',\n",
       " 'O kadar uğraşmasına rağmen bazen çok iyi sonuçlar elde edemezdi',\n",
       " 'Yeterince iyi olmadıkları için bu sonuçlardan Sonyaya ve tabii ki Madeleinee bahsetmezdi',\n",
       " 'Gözlerin sırrı gerilimi ve çelişkiyi her şeyden daha iyi yakalayabilmesindeydi']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences[70:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert characters to integers\n",
    "voc2int = {}\n",
    "ch_count = 0\n",
    "for sentence in cleaned_sentences:\n",
    "    for character in sentence:\n",
    "        if character not in voc2int:\n",
    "            voc2int[character] = ch_count\n",
    "            ch_count += 1\n",
    "\n",
    "# Add special tokens to voc2int\n",
    "# PAD - Padding\n",
    "# EOS - End of sentence\n",
    "# GO  - Start of the sentence\n",
    "tokens = ['<PAD>','<EOS>','<GO>']\n",
    "for token in tokens:\n",
    "    voc2int[token] = ch_count\n",
    "    ch_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vocabulary: 98\n",
      "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<EOS>', '<GO>', '<PAD>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Â', 'Å', 'Ç', 'É', 'Î', 'Ö', 'Û', 'Ü', 'à', 'á', 'â', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ó', 'ô', 'ö', 'ù', 'ú', 'û', 'ü', 'Ğ', 'ğ', 'İ', 'ı', 'Ş', 'ş']\n"
     ]
    }
   ],
   "source": [
    "print('Length of the vocabulary: ' + str(len(voc2int)))\n",
    "print(sorted(voc2int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İ\n"
     ]
    }
   ],
   "source": [
    "# Convert integers to characters\n",
    "int2voc = {}\n",
    "for character, value in voc2int.items():\n",
    "    int2voc[value] = character\n",
    "print(int2voc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to integers\n",
    "def sentence_to_integer(sentence):\n",
    "    int_sentence = []\n",
    "    for character in sentence:\n",
    "        int_sentence.append(voc2int[character])\n",
    "    return int_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not şöyle devam etmektedir Sırlarını nasıl bildiğimi göreceksin\n",
      "[2, 30, 13, 8, 24, 40, 38, 19, 20, 8, 37, 20, 16, 10, 14, 8, 20, 13, 14, 20, 33, 13, 20, 37, 23, 21, 8, 36, 12, 21, 19, 10, 21, 12, 11, 12, 8, 11, 10, 27, 12, 19, 8, 34, 23, 19, 37, 23, 39, 23, 14, 23, 8, 17, 40, 21, 20, 26, 20, 33, 27, 23, 11]\n"
     ]
    }
   ],
   "source": [
    "# Convert sentences to integers\n",
    "sent2int = []\n",
    "\n",
    "for sentence in cleaned_sentences:\n",
    "    sent2int.append(sentence_to_integer(sentence))\n",
    "\n",
    "print(cleaned_sentences[3])\n",
    "print(sent2int[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>308954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>67.392036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>67.771632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>89.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5586.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              counts\n",
       "count  308954.000000\n",
       "mean       67.392036\n",
       "std        67.771632\n",
       "min         0.000000\n",
       "25%        28.000000\n",
       "50%        52.000000\n",
       "75%        89.000000\n",
       "max      5586.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find length of the sentences\n",
    "lengths = []\n",
    "for sentence in sent2int:\n",
    "    lengths.append(len(sentence))\n",
    "lengths = pd.DataFrame(lengths, columns=[\"counts\"])\n",
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filtered sentences to train: 102925\n"
     ]
    }
   ],
   "source": [
    "# Limitation of the training data\n",
    "max_length = 45\n",
    "min_length = 15\n",
    "\n",
    "filtered_sentences = []\n",
    "\n",
    "for sentence in sent2int:\n",
    "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
    "        filtered_sentences.append(sentence)\n",
    "print(\"Total filtered sentences to train: \" + str(len(filtered_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 92632\n",
      "Number of testing sentences: 10293\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sentences\n",
    "train, test = train_test_split(filtered_sentences, test_size = 0.1, random_state = 53)\n",
    "print(\"Number of training sentences:\", len(train))\n",
    "print(\"Number of testing sentences:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we sort the sentences the batches will include similar length sentences, so less padding will be used.\n",
    "# Hence, the model will train faster.\n",
    "sorted_train = []\n",
    "sorted_test = []\n",
    "for i in range(min_length, max_length+1):\n",
    "    for sentence in train:\n",
    "        if len(sentence) == i:\n",
    "            sorted_train.append(sentence)\n",
    "    for sentence in test:\n",
    "        if len(sentence) == i:\n",
    "            sorted_test.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "letters = ['A', 'B', 'C', 'Ç', 'D', 'E', 'F', 'G', 'Ğ', 'H', 'I',\n",
    "           'İ', 'J', 'K', 'L', 'M', 'N', 'O', 'Ö', 'P', 'Q', 'R', \n",
    "           'S', 'Ş', 'T', 'U', 'Ü', 'V', 'W', 'X', 'Y', 'Z',\n",
    "\"\"\"\n",
    "           \n",
    "letters = ['a', 'b', 'c', 'ç', 'd', 'e', 'f', 'g', 'ğ', 'h', 'ı', 'i', \n",
    "           'j', 'k', 'l', 'm', 'n', 'o', 'ö', 'p', 'q', 'r', 's', \n",
    "           'ş', 't', 'u', 'ü', 'v', 'w', 'x', 'y' ,'z']\n",
    "\n",
    "# Creates spelling mistakes by adding, relocating and removing characters\n",
    "def create_noise(sentence, threshold):\n",
    "    noisy_sentence = []\n",
    "    ch = 0\n",
    "    while ch < len(sentence):\n",
    "        random = np.random.random()\n",
    "        # If threshold close to 0 do not change, if it is close to 1 change everything\n",
    "        if random > threshold:\n",
    "            noisy_sentence.append(sentence[ch])\n",
    "        else:\n",
    "            random = np.random.random()\n",
    "            # 25% chance characters will swap to each other\n",
    "            if random >= 0.75:\n",
    "                # If it is last character\n",
    "                if ch == (len(sentence) - 1):\n",
    "                    continue\n",
    "                else:\n",
    "                    noisy_sentence.append(sentence[ch+1])\n",
    "                    noisy_sentence.append(sentence[ch])\n",
    "                    ch += 1\n",
    "            # 25% chance to add an extra letter\n",
    "            elif random >= 0.5:\n",
    "                extra_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(sentence[ch])\n",
    "                noisy_sentence.append(voc2int[extra_letter])\n",
    "            # 25% chance the letter is missing\n",
    "            elif random >= 0.25:\n",
    "                pass\n",
    "            # 25% chance change with wrong letter\n",
    "            elif random >= 0:\n",
    "                wrong_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(voc2int[wrong_letter])\n",
    "        ch +=1\n",
    "    return noisy_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate int sentence to string sentence\n",
    "def translate_sentence(sentence):\n",
    "    translated_sentence = \"\"\n",
    "    for character in sentence:\n",
    "        translated_sentence += int2voc[character]\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene tıraşı uzamıştı\n",
      "Gene tıraşı uzamrıtşı\n",
      "Çok oluyor mu geleli\n",
      "Çok oluyor mu egleli\n",
      "Işığa gerek yoktu ki\n",
      "Işığa gerek yoktu ki\n",
      "Süreyya deli gibiydi\n",
      "Süreyy adeoli gibiydi\n",
      "Oysa ne çok şey oldu\n",
      "Oisa ne çok şey old\n"
     ]
    }
   ],
   "source": [
    "# Example mistakes\n",
    "\n",
    "# 10% chance to make spelling mistake for each letter\n",
    "threshold = 0.05\n",
    "for sentence in sorted_train[15000:15005]:\n",
    "    wrong_sentence = create_noise(sentence, threshold)\n",
    "    print(translate_sentence(sentence))\n",
    "    print(translate_sentence(wrong_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding to the sentences to have same length\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    # Find max length in this batch\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [voc2int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "def get_batches(sorted_sentences, batch_size, threshold):\n",
    "\n",
    "    for batch_no in range(len(sorted_sentences)//batch_size):\n",
    "        start_no = batch_no * batch_size\n",
    "        sentences_batch = copy.deepcopy(sorted_sentences[start_no:start_no + batch_size])\n",
    "        sentences_batch_copy = copy.deepcopy(sorted_sentences[start_no:start_no + batch_size])\n",
    "        \n",
    "        sentences_batch_noisy = []\n",
    "        for sentence in sentences_batch:\n",
    "            sentence = create_noise(sentence, threshold)\n",
    "            sentence.append(voc2int['<EOS>'])\n",
    "            sentence.insert(0,voc2int['<GO>'])            \n",
    "            sentences_batch_noisy.append(sentence)\n",
    "\n",
    "            \n",
    "        sentences_batch_final = []\n",
    "        \n",
    "        # Add EOS token\n",
    "        for sentence in sentences_batch_copy:\n",
    "            sentence.append(voc2int['<EOS>'])\n",
    "            sentence.insert(0,voc2int['<GO>'])\n",
    "            sentences_batch_final.append(sentence)\n",
    "            \n",
    "        \n",
    "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_final))\n",
    "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
    "        \n",
    "        yield pad_sentences_noisy_batch, pad_sentences_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inp_size = len(voc2int)+1\n",
    "vocab_tar_size = len(voc2int)+1\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "threshold = 0.075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(get_batches(sorted_train, batch_size, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 21, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
    "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 21, 1)\n"
     ]
    }
   ],
   "source": [
    "# sample attention\n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 99)\n"
     ]
    }
   ],
   "source": [
    "# sample output\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, batch_size)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([voc2int['<GO>']] * batch_size, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Train Loss 4.3212 Time 0.52 sec\n",
      "Epoch 1 Batch 100 Train Loss 2.8294 Time 27.37 sec\n",
      "Epoch 1 Batch 200 Train Loss 2.4438 Time 31.04 sec\n",
      "Epoch 1 Batch 300 Train Loss 2.4652 Time 34.60 sec\n",
      "Epoch 1 Batch 400 Train Loss 2.3584 Time 38.56 sec\n",
      "Epoch 1 Batch 500 Train Loss 2.3165 Time 42.94 sec\n",
      "Epoch 1 Batch 600 Train Loss 2.1291 Time 45.05 sec\n",
      "Epoch 1 Batch 700 Train Loss 2.1496 Time 48.29 sec\n",
      "Epoch 1 Batch 800 Train Loss 1.9565 Time 54.14 sec\n",
      "Epoch 1 Batch 900 Train Loss 2.3416 Time 55.83 sec\n",
      "Epoch 1 Batch 1000 Train Loss 1.9970 Time 59.13 sec\n",
      "Epoch 1 Batch 1100 Train Loss 1.7601 Time 62.52 sec\n",
      "Epoch 1 Batch 1200 Train Loss 1.3003 Time 66.11 sec\n",
      "Epoch 1 Batch 1300 Train Loss 0.7466 Time 71.83 sec\n",
      "Epoch 1 Batch 1400 Train Loss 0.7217 Time 74.73 sec\n",
      "\n",
      "**Epoch 1 Test Loss 0.9408 \n",
      "\n",
      "**Epoch 1 Train Loss 2.0145\n",
      "Time taken for 1 epoch 831.42 sec\n",
      "\n",
      "Epoch 2 Batch 0 Train Loss 2.6911 Time 0.27 sec\n",
      "Epoch 2 Batch 100 Train Loss 2.1131 Time 28.57 sec\n",
      "Epoch 2 Batch 200 Train Loss 1.9334 Time 32.63 sec\n",
      "Epoch 2 Batch 300 Train Loss 1.7988 Time 35.75 sec\n",
      "Epoch 2 Batch 400 Train Loss 1.6656 Time 38.44 sec\n",
      "Epoch 2 Batch 500 Train Loss 1.2627 Time 41.58 sec\n",
      "Epoch 2 Batch 600 Train Loss 0.7129 Time 45.53 sec\n",
      "Epoch 2 Batch 700 Train Loss 0.7411 Time 49.00 sec\n",
      "Epoch 2 Batch 800 Train Loss 0.5641 Time 54.72 sec\n",
      "Epoch 2 Batch 900 Train Loss 0.7588 Time 58.37 sec\n",
      "Epoch 2 Batch 1000 Train Loss 0.5707 Time 59.72 sec\n",
      "Epoch 2 Batch 1100 Train Loss 1.8594 Time 63.26 sec\n",
      "Epoch 2 Batch 1200 Train Loss 0.6841 Time 67.11 sec\n",
      "Epoch 2 Batch 1300 Train Loss 0.6381 Time 70.83 sec\n",
      "Epoch 2 Batch 1400 Train Loss 0.5753 Time 76.58 sec\n",
      "\n",
      "**Epoch 2 Test Loss 0.4731 \n",
      "\n",
      "**Epoch 2 Train Loss 1.1066\n",
      "Time taken for 1 epoch 842.11 sec\n",
      "\n",
      "Epoch 3 Batch 0 Train Loss 0.4070 Time 0.28 sec\n",
      "Epoch 3 Batch 100 Train Loss 0.4563 Time 28.73 sec\n",
      "Epoch 3 Batch 200 Train Loss 0.3403 Time 32.16 sec\n",
      "Epoch 3 Batch 300 Train Loss 0.3875 Time 35.39 sec\n",
      "Epoch 3 Batch 400 Train Loss 0.4239 Time 39.17 sec\n",
      "Epoch 3 Batch 500 Train Loss 0.4073 Time 42.46 sec\n",
      "Epoch 3 Batch 600 Train Loss 0.3713 Time 45.78 sec\n",
      "Epoch 3 Batch 700 Train Loss 0.4139 Time 49.14 sec\n",
      "Epoch 3 Batch 800 Train Loss 0.5005 Time 52.39 sec\n",
      "Epoch 3 Batch 900 Train Loss 0.4054 Time 55.68 sec\n",
      "Epoch 3 Batch 1000 Train Loss 0.3782 Time 59.21 sec\n",
      "Epoch 3 Batch 1100 Train Loss 0.3397 Time 62.99 sec\n",
      "Epoch 3 Batch 1200 Train Loss 0.4339 Time 66.92 sec\n",
      "Epoch 3 Batch 1300 Train Loss 0.4342 Time 70.03 sec\n",
      "Epoch 3 Batch 1400 Train Loss 0.4084 Time 73.69 sec\n",
      "\n",
      "**Epoch 3 Test Loss 0.3612 \n",
      "\n",
      "**Epoch 3 Train Loss 0.4015\n",
      "Time taken for 1 epoch 833.07 sec\n",
      "\n",
      "Epoch 4 Batch 0 Train Loss 0.3460 Time 0.30 sec\n",
      "Epoch 4 Batch 100 Train Loss 0.3541 Time 28.80 sec\n",
      "Epoch 4 Batch 200 Train Loss 0.2922 Time 32.13 sec\n",
      "Epoch 4 Batch 300 Train Loss 0.4841 Time 35.42 sec\n",
      "Epoch 4 Batch 400 Train Loss 0.3911 Time 38.97 sec\n",
      "Epoch 4 Batch 500 Train Loss 0.3167 Time 42.24 sec\n",
      "Epoch 4 Batch 600 Train Loss 0.3053 Time 46.29 sec\n",
      "Epoch 4 Batch 700 Train Loss 0.3800 Time 49.23 sec\n",
      "Epoch 4 Batch 800 Train Loss 0.3155 Time 52.72 sec\n",
      "Epoch 4 Batch 900 Train Loss 0.3786 Time 56.50 sec\n",
      "Epoch 4 Batch 1000 Train Loss 0.5192 Time 60.26 sec\n",
      "Epoch 4 Batch 1100 Train Loss 0.3776 Time 62.72 sec\n",
      "Epoch 4 Batch 1200 Train Loss 0.6554 Time 66.40 sec\n",
      "Epoch 4 Batch 1300 Train Loss 0.4701 Time 70.24 sec\n",
      "Epoch 4 Batch 1400 Train Loss 0.4364 Time 74.92 sec\n",
      "\n",
      "**Epoch 4 Test Loss 0.3578 \n",
      "\n",
      "**Epoch 4 Train Loss 0.4223\n",
      "Time taken for 1 epoch 836.25 sec\n",
      "\n",
      "Epoch 5 Batch 0 Train Loss 0.3478 Time 0.26 sec\n",
      "Epoch 5 Batch 100 Train Loss 0.3179 Time 28.70 sec\n",
      "Epoch 5 Batch 200 Train Loss 0.3167 Time 32.10 sec\n",
      "Epoch 5 Batch 300 Train Loss 0.3311 Time 35.65 sec\n",
      "Epoch 5 Batch 400 Train Loss 0.3652 Time 38.59 sec\n",
      "Epoch 5 Batch 500 Train Loss 0.3414 Time 42.02 sec\n",
      "Epoch 5 Batch 600 Train Loss 0.3707 Time 45.57 sec\n",
      "Epoch 5 Batch 700 Train Loss 0.3463 Time 53.82 sec\n",
      "Epoch 5 Batch 800 Train Loss 0.3384 Time 54.21 sec\n",
      "Epoch 5 Batch 900 Train Loss 0.3350 Time 62.54 sec\n",
      "Epoch 5 Batch 1000 Train Loss 0.3034 Time 63.39 sec\n",
      "Epoch 5 Batch 1100 Train Loss 0.2826 Time 66.00 sec\n",
      "Epoch 5 Batch 1200 Train Loss 0.4166 Time 70.00 sec\n",
      "Epoch 5 Batch 1300 Train Loss 0.3269 Time 74.36 sec\n",
      "Epoch 5 Batch 1400 Train Loss 0.3359 Time 78.59 sec\n",
      "\n",
      "**Epoch 5 Test Loss 0.9034 \n",
      "\n",
      "**Epoch 5 Train Loss 0.3265\n",
      "Time taken for 1 epoch 871.98 sec\n",
      "\n",
      "Epoch 6 Batch 0 Train Loss 0.3413 Time 0.28 sec\n",
      "Epoch 6 Batch 100 Train Loss 0.3472 Time 30.57 sec\n",
      "Epoch 6 Batch 200 Train Loss 0.3574 Time 34.01 sec\n",
      "Epoch 6 Batch 300 Train Loss 0.4017 Time 37.69 sec\n",
      "Epoch 6 Batch 400 Train Loss 0.3644 Time 41.27 sec\n",
      "Epoch 6 Batch 500 Train Loss 0.2932 Time 44.91 sec\n",
      "Epoch 6 Batch 600 Train Loss 0.3162 Time 48.87 sec\n",
      "Epoch 6 Batch 700 Train Loss 0.2931 Time 53.06 sec\n",
      "Epoch 6 Batch 800 Train Loss 0.2972 Time 55.62 sec\n",
      "Epoch 6 Batch 900 Train Loss 0.3550 Time 59.51 sec\n",
      "Epoch 6 Batch 1000 Train Loss 0.5842 Time 62.96 sec\n",
      "Epoch 6 Batch 1100 Train Loss 0.3605 Time 66.61 sec\n",
      "Epoch 6 Batch 1200 Train Loss 0.6086 Time 70.50 sec\n",
      "Epoch 6 Batch 1300 Train Loss 0.4419 Time 73.93 sec\n",
      "Epoch 6 Batch 1400 Train Loss 0.3384 Time 80.64 sec\n",
      "\n",
      "**Epoch 6 Test Loss 0.4106 \n",
      "\n",
      "**Epoch 6 Train Loss 0.3859\n",
      "Time taken for 1 epoch 881.87 sec\n",
      "\n",
      "Epoch 7 Batch 0 Train Loss 0.3569 Time 0.27 sec\n",
      "Epoch 7 Batch 100 Train Loss 0.3172 Time 32.01 sec\n",
      "Epoch 7 Batch 200 Train Loss 0.3542 Time 36.18 sec\n",
      "Epoch 7 Batch 300 Train Loss 0.3046 Time 47.02 sec\n",
      "Epoch 7 Batch 400 Train Loss 0.3464 Time 48.11 sec\n",
      "Epoch 7 Batch 500 Train Loss 0.3117 Time 52.19 sec\n",
      "Epoch 7 Batch 600 Train Loss 0.2769 Time 59.64 sec\n",
      "Epoch 7 Batch 700 Train Loss 0.3256 Time 61.65 sec\n",
      "Epoch 7 Batch 800 Train Loss 0.3143 Time 67.51 sec\n",
      "Epoch 7 Batch 900 Train Loss 0.3465 Time 67.31 sec\n",
      "Epoch 7 Batch 1000 Train Loss 0.2992 Time 73.47 sec\n",
      "Epoch 7 Batch 1100 Train Loss 0.2968 Time 75.47 sec\n",
      "Epoch 7 Batch 1200 Train Loss 0.2913 Time 84.98 sec\n",
      "Epoch 7 Batch 1300 Train Loss 1.4727 Time 88.09 sec\n",
      "Epoch 7 Batch 1400 Train Loss 0.9922 Time 91.35 sec\n",
      "\n",
      "**Epoch 7 Test Loss 0.4247 \n",
      "\n",
      "**Epoch 7 Train Loss 0.3834\n",
      "Time taken for 1 epoch 1035.19 sec\n",
      "\n",
      "Epoch 8 Batch 0 Train Loss 0.2365 Time 0.32 sec\n",
      "Epoch 8 Batch 100 Train Loss 0.3856 Time 32.90 sec\n",
      "Epoch 8 Batch 200 Train Loss 0.3171 Time 32.86 sec\n",
      "Epoch 8 Batch 300 Train Loss 0.3248 Time 35.59 sec\n",
      "Epoch 8 Batch 400 Train Loss 0.3046 Time 39.12 sec\n",
      "Epoch 8 Batch 500 Train Loss 0.3351 Time 42.13 sec\n",
      "Epoch 8 Batch 600 Train Loss 0.2767 Time 45.81 sec\n",
      "Epoch 8 Batch 700 Train Loss 0.3097 Time 50.69 sec\n",
      "Epoch 8 Batch 800 Train Loss 0.2870 Time 52.77 sec\n",
      "Epoch 8 Batch 900 Train Loss 0.3124 Time 55.68 sec\n",
      "Epoch 8 Batch 1000 Train Loss 0.2993 Time 59.58 sec\n",
      "Epoch 8 Batch 1100 Train Loss 0.2687 Time 63.46 sec\n",
      "Epoch 8 Batch 1200 Train Loss 0.2840 Time 76.44 sec\n",
      "Epoch 8 Batch 1300 Train Loss 0.3662 Time 77.74 sec\n",
      "Epoch 8 Batch 1400 Train Loss 0.2850 Time 79.24 sec\n",
      "\n",
      "**Epoch 8 Test Loss 0.3566 \n",
      "\n",
      "**Epoch 8 Train Loss 0.3176\n",
      "Time taken for 1 epoch 871.23 sec\n",
      "\n",
      "Epoch 9 Batch 0 Train Loss 0.3208 Time 0.29 sec\n",
      "Epoch 9 Batch 100 Train Loss 0.3212 Time 29.01 sec\n",
      "Epoch 9 Batch 200 Train Loss 0.2752 Time 34.01 sec\n",
      "Epoch 9 Batch 300 Train Loss 0.2735 Time 39.81 sec\n",
      "Epoch 9 Batch 400 Train Loss 0.2329 Time 40.22 sec\n",
      "Epoch 9 Batch 500 Train Loss 0.2818 Time 46.03 sec\n",
      "Epoch 9 Batch 600 Train Loss 0.2626 Time 52.39 sec\n",
      "Epoch 9 Batch 700 Train Loss 0.3329 Time 53.73 sec\n",
      "Epoch 9 Batch 800 Train Loss 0.2989 Time 56.92 sec\n",
      "Epoch 9 Batch 900 Train Loss 0.3507 Time 60.48 sec\n",
      "Epoch 9 Batch 1000 Train Loss 0.3278 Time 64.59 sec\n",
      "Epoch 9 Batch 1100 Train Loss 0.3032 Time 67.17 sec\n",
      "Epoch 9 Batch 1200 Train Loss 0.3281 Time 68.21 sec\n",
      "Epoch 9 Batch 1300 Train Loss 0.2698 Time 71.77 sec\n",
      "Epoch 9 Batch 1400 Train Loss 0.2692 Time 75.48 sec\n",
      "\n",
      "**Epoch 9 Test Loss 0.2752 \n",
      "\n",
      "**Epoch 9 Train Loss 0.3031\n",
      "Time taken for 1 epoch 888.61 sec\n",
      "\n",
      "Epoch 10 Batch 0 Train Loss 0.2646 Time 0.33 sec\n",
      "Epoch 10 Batch 100 Train Loss 0.2743 Time 31.73 sec\n",
      "Epoch 10 Batch 200 Train Loss 0.2474 Time 34.23 sec\n",
      "Epoch 10 Batch 300 Train Loss 0.2344 Time 37.44 sec\n",
      "Epoch 10 Batch 400 Train Loss 0.2518 Time 41.80 sec\n",
      "Epoch 10 Batch 500 Train Loss 0.2757 Time 46.56 sec\n",
      "Epoch 10 Batch 600 Train Loss 0.3144 Time 48.21 sec\n",
      "Epoch 10 Batch 700 Train Loss 0.3201 Time 51.79 sec\n",
      "Epoch 10 Batch 800 Train Loss 0.2739 Time 55.02 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 900 Train Loss 0.2749 Time 57.53 sec\n",
      "Epoch 10 Batch 1000 Train Loss 0.2256 Time 62.05 sec\n",
      "Epoch 10 Batch 1100 Train Loss 0.2323 Time 65.06 sec\n",
      "Epoch 10 Batch 1200 Train Loss 0.2519 Time 71.15 sec\n",
      "Epoch 10 Batch 1300 Train Loss 0.2620 Time 76.97 sec\n",
      "Epoch 10 Batch 1400 Train Loss 1.7715 Time 80.90 sec\n",
      "\n",
      "**Epoch 10 Test Loss 0.3921 \n",
      "\n",
      "**Epoch 10 Train Loss 0.3173\n",
      "Time taken for 1 epoch 890.04 sec\n",
      "\n",
      "Epoch 11 Batch 0 Train Loss 0.3437 Time 0.28 sec\n",
      "Epoch 11 Batch 100 Train Loss 0.3001 Time 29.37 sec\n",
      "Epoch 11 Batch 200 Train Loss 0.2982 Time 32.82 sec\n",
      "Epoch 11 Batch 300 Train Loss 0.2185 Time 35.43 sec\n",
      "Epoch 11 Batch 400 Train Loss 0.2594 Time 38.75 sec\n",
      "Epoch 11 Batch 500 Train Loss 0.2355 Time 41.97 sec\n",
      "Epoch 11 Batch 600 Train Loss 0.2745 Time 45.91 sec\n",
      "Epoch 11 Batch 700 Train Loss 0.2747 Time 49.44 sec\n",
      "Epoch 11 Batch 800 Train Loss 0.2603 Time 53.75 sec\n",
      "Epoch 11 Batch 900 Train Loss 0.2225 Time 56.21 sec\n",
      "Epoch 11 Batch 1000 Train Loss 0.2585 Time 59.14 sec\n",
      "Epoch 11 Batch 1100 Train Loss 1.1448 Time 62.64 sec\n",
      "Epoch 11 Batch 1200 Train Loss 0.4484 Time 66.05 sec\n",
      "Epoch 11 Batch 1300 Train Loss 0.3416 Time 69.66 sec\n",
      "Epoch 11 Batch 1400 Train Loss 0.4103 Time 73.64 sec\n",
      "\n",
      "**Epoch 11 Test Loss 0.3111 \n",
      "\n",
      "**Epoch 11 Train Loss 0.3293\n",
      "Time taken for 1 epoch 833.40 sec\n",
      "\n",
      "Epoch 12 Batch 0 Train Loss 0.2580 Time 0.25 sec\n",
      "Epoch 12 Batch 100 Train Loss 0.3450 Time 28.71 sec\n",
      "Epoch 12 Batch 200 Train Loss 0.2274 Time 32.15 sec\n",
      "Epoch 12 Batch 300 Train Loss 0.2282 Time 35.40 sec\n",
      "Epoch 12 Batch 400 Train Loss 0.2540 Time 38.55 sec\n",
      "Epoch 12 Batch 500 Train Loss 0.2333 Time 41.77 sec\n",
      "Epoch 12 Batch 600 Train Loss 0.2648 Time 45.08 sec\n",
      "Epoch 12 Batch 700 Train Loss 0.2764 Time 48.93 sec\n",
      "Epoch 12 Batch 800 Train Loss 0.9721 Time 51.81 sec\n",
      "Epoch 12 Batch 900 Train Loss 0.3590 Time 55.22 sec\n",
      "Epoch 12 Batch 1000 Train Loss 0.2916 Time 58.52 sec\n",
      "Epoch 12 Batch 1100 Train Loss 0.2852 Time 62.12 sec\n",
      "Epoch 12 Batch 1200 Train Loss 0.3404 Time 65.60 sec\n",
      "Epoch 12 Batch 1300 Train Loss 0.2561 Time 69.43 sec\n",
      "Epoch 12 Batch 1400 Train Loss 0.2825 Time 73.31 sec\n",
      "\n",
      "**Epoch 12 Test Loss 0.2712 \n",
      "\n",
      "**Epoch 12 Train Loss 0.3670\n",
      "Time taken for 1 epoch 824.35 sec\n",
      "\n",
      "Epoch 13 Batch 0 Train Loss 0.2559 Time 0.30 sec\n",
      "Epoch 13 Batch 100 Train Loss 0.2910 Time 28.31 sec\n",
      "Epoch 13 Batch 200 Train Loss 0.2577 Time 31.65 sec\n",
      "Epoch 13 Batch 300 Train Loss 0.2442 Time 34.96 sec\n",
      "Epoch 13 Batch 400 Train Loss 0.2786 Time 38.32 sec\n",
      "Epoch 13 Batch 500 Train Loss 0.2575 Time 41.44 sec\n",
      "Epoch 13 Batch 600 Train Loss 0.2348 Time 44.79 sec\n",
      "Epoch 13 Batch 700 Train Loss 0.2948 Time 48.40 sec\n",
      "Epoch 13 Batch 800 Train Loss 0.2370 Time 52.02 sec\n",
      "Epoch 13 Batch 900 Train Loss 0.2645 Time 55.05 sec\n",
      "Epoch 13 Batch 1000 Train Loss 0.2268 Time 58.58 sec\n",
      "Epoch 13 Batch 1100 Train Loss 0.2278 Time 62.00 sec\n",
      "Epoch 13 Batch 1200 Train Loss 0.2564 Time 65.59 sec\n",
      "Epoch 13 Batch 1300 Train Loss 0.2708 Time 68.92 sec\n",
      "Epoch 13 Batch 1400 Train Loss 0.4600 Time 72.88 sec\n",
      "\n",
      "**Epoch 13 Test Loss 0.3238 \n",
      "\n",
      "**Epoch 13 Train Loss 0.2993\n",
      "Time taken for 1 epoch 819.92 sec\n",
      "\n",
      "Epoch 14 Batch 0 Train Loss 0.3244 Time 0.28 sec\n",
      "Epoch 14 Batch 100 Train Loss 0.2597 Time 28.33 sec\n",
      "Epoch 14 Batch 200 Train Loss 0.2536 Time 31.62 sec\n",
      "Epoch 14 Batch 300 Train Loss 0.2744 Time 35.04 sec\n",
      "Epoch 14 Batch 400 Train Loss 0.3864 Time 38.27 sec\n",
      "Epoch 14 Batch 500 Train Loss 0.2769 Time 41.41 sec\n",
      "Epoch 14 Batch 600 Train Loss 0.2373 Time 45.04 sec\n",
      "Epoch 14 Batch 700 Train Loss 0.2598 Time 48.51 sec\n",
      "Epoch 14 Batch 800 Train Loss 0.2428 Time 51.81 sec\n",
      "Epoch 14 Batch 900 Train Loss 0.2913 Time 54.95 sec\n",
      "Epoch 14 Batch 1000 Train Loss 0.2772 Time 59.30 sec\n",
      "Epoch 14 Batch 1100 Train Loss 0.2467 Time 61.94 sec\n",
      "Epoch 14 Batch 1200 Train Loss 0.3014 Time 65.48 sec\n",
      "Epoch 14 Batch 1300 Train Loss 0.2609 Time 69.06 sec\n",
      "Epoch 14 Batch 1400 Train Loss 0.2329 Time 73.05 sec\n",
      "\n",
      "**Epoch 14 Test Loss 0.2369 \n",
      "\n",
      "**Epoch 14 Train Loss 0.2687\n",
      "Time taken for 1 epoch 820.89 sec\n",
      "\n",
      "Epoch 15 Batch 0 Train Loss 0.2448 Time 0.26 sec\n",
      "Epoch 15 Batch 100 Train Loss 0.2648 Time 28.22 sec\n",
      "Epoch 15 Batch 200 Train Loss 0.2409 Time 31.75 sec\n",
      "Epoch 15 Batch 300 Train Loss 0.2225 Time 35.83 sec\n",
      "Epoch 15 Batch 400 Train Loss 0.2405 Time 38.35 sec\n",
      "Epoch 15 Batch 500 Train Loss 0.2519 Time 41.29 sec\n",
      "Epoch 15 Batch 600 Train Loss 0.2400 Time 45.80 sec\n",
      "Epoch 15 Batch 700 Train Loss 0.2282 Time 50.33 sec\n",
      "Epoch 15 Batch 800 Train Loss 0.2687 Time 52.38 sec\n",
      "Epoch 15 Batch 900 Train Loss 0.2105 Time 55.36 sec\n",
      "Epoch 15 Batch 1000 Train Loss 0.2086 Time 58.50 sec\n",
      "Epoch 15 Batch 1100 Train Loss 0.2135 Time 61.77 sec\n",
      "Epoch 15 Batch 1200 Train Loss 0.2233 Time 65.67 sec\n",
      "Epoch 15 Batch 1300 Train Loss 0.2616 Time 70.01 sec\n",
      "Epoch 15 Batch 1400 Train Loss 0.5261 Time 73.09 sec\n",
      "\n",
      "**Epoch 15 Test Loss 0.2901 \n",
      "\n",
      "**Epoch 15 Train Loss 0.2763\n",
      "Time taken for 1 epoch 825.61 sec\n",
      "\n",
      "Epoch 16 Batch 0 Train Loss 0.2763 Time 0.27 sec\n",
      "Epoch 16 Batch 100 Train Loss 0.2703 Time 28.42 sec\n",
      "Epoch 16 Batch 200 Train Loss 0.1863 Time 31.61 sec\n",
      "Epoch 16 Batch 300 Train Loss 0.2083 Time 34.78 sec\n",
      "Epoch 16 Batch 400 Train Loss 0.2427 Time 38.19 sec\n",
      "Epoch 16 Batch 500 Train Loss 0.2056 Time 41.39 sec\n",
      "Epoch 16 Batch 600 Train Loss 0.1730 Time 44.81 sec\n",
      "Epoch 16 Batch 700 Train Loss 0.2402 Time 48.34 sec\n",
      "Epoch 16 Batch 800 Train Loss 0.2505 Time 51.89 sec\n",
      "Epoch 16 Batch 900 Train Loss 0.2818 Time 55.15 sec\n",
      "Epoch 16 Batch 1000 Train Loss 0.2550 Time 58.26 sec\n",
      "Epoch 16 Batch 1100 Train Loss 0.2498 Time 61.76 sec\n",
      "Epoch 16 Batch 1200 Train Loss 1.8433 Time 65.50 sec\n",
      "Epoch 16 Batch 1300 Train Loss 0.4281 Time 69.20 sec\n",
      "Epoch 16 Batch 1400 Train Loss 0.3225 Time 73.03 sec\n",
      "\n",
      "**Epoch 16 Test Loss 0.3029 \n",
      "\n",
      "**Epoch 16 Train Loss 0.3339\n",
      "Time taken for 1 epoch 819.29 sec\n",
      "\n",
      "Epoch 17 Batch 0 Train Loss 0.2351 Time 0.27 sec\n",
      "Epoch 17 Batch 100 Train Loss 0.3016 Time 28.32 sec\n",
      "Epoch 17 Batch 200 Train Loss 0.2151 Time 31.62 sec\n",
      "Epoch 17 Batch 300 Train Loss 0.2419 Time 34.96 sec\n",
      "Epoch 17 Batch 400 Train Loss 0.2130 Time 38.49 sec\n",
      "Epoch 17 Batch 500 Train Loss 0.1962 Time 45.65 sec\n",
      "Epoch 17 Batch 600 Train Loss 0.3256 Time 50.89 sec\n",
      "Epoch 17 Batch 700 Train Loss 0.2514 Time 48.76 sec\n",
      "Epoch 17 Batch 800 Train Loss 0.3418 Time 52.12 sec\n",
      "Epoch 17 Batch 900 Train Loss 0.2739 Time 55.16 sec\n",
      "Epoch 17 Batch 1000 Train Loss 0.2468 Time 58.80 sec\n",
      "Epoch 17 Batch 1100 Train Loss 0.2574 Time 61.94 sec\n",
      "Epoch 17 Batch 1200 Train Loss 0.4079 Time 65.37 sec\n",
      "Epoch 17 Batch 1300 Train Loss 0.3009 Time 69.46 sec\n",
      "Epoch 17 Batch 1400 Train Loss 0.2888 Time 73.21 sec\n",
      "\n",
      "**Epoch 17 Test Loss 0.3394 \n",
      "\n",
      "**Epoch 17 Train Loss 0.2708\n",
      "Time taken for 1 epoch 832.38 sec\n",
      "\n",
      "Epoch 18 Batch 0 Train Loss 0.2463 Time 0.30 sec\n",
      "Epoch 18 Batch 100 Train Loss 0.2883 Time 28.36 sec\n",
      "Epoch 18 Batch 200 Train Loss 0.2576 Time 31.54 sec\n",
      "Epoch 18 Batch 300 Train Loss 0.1972 Time 34.90 sec\n",
      "Epoch 18 Batch 400 Train Loss 0.2403 Time 38.18 sec\n",
      "Epoch 18 Batch 500 Train Loss 0.2468 Time 41.51 sec\n",
      "Epoch 18 Batch 600 Train Loss 0.2138 Time 45.24 sec\n",
      "Epoch 18 Batch 700 Train Loss 0.3229 Time 49.04 sec\n",
      "Epoch 18 Batch 800 Train Loss 0.2843 Time 51.73 sec\n",
      "Epoch 18 Batch 900 Train Loss 0.1945 Time 55.13 sec\n",
      "Epoch 18 Batch 1000 Train Loss 0.2307 Time 58.18 sec\n",
      "Epoch 18 Batch 1100 Train Loss 0.2798 Time 61.89 sec\n",
      "Epoch 18 Batch 1200 Train Loss 0.2408 Time 65.31 sec\n",
      "Epoch 18 Batch 1300 Train Loss 0.2559 Time 68.80 sec\n",
      "Epoch 18 Batch 1400 Train Loss 0.2747 Time 73.08 sec\n",
      "\n",
      "**Epoch 18 Test Loss 0.2236 \n",
      "\n",
      "**Epoch 18 Train Loss 0.2378\n",
      "Time taken for 1 epoch 821.00 sec\n",
      "\n",
      "Epoch 19 Batch 0 Train Loss 0.1864 Time 0.27 sec\n",
      "Epoch 19 Batch 100 Train Loss 0.2541 Time 28.34 sec\n",
      "Epoch 19 Batch 200 Train Loss 0.1794 Time 31.79 sec\n",
      "Epoch 19 Batch 300 Train Loss 0.2270 Time 34.97 sec\n",
      "Epoch 19 Batch 400 Train Loss 0.2439 Time 38.16 sec\n",
      "Epoch 19 Batch 500 Train Loss 0.2299 Time 41.48 sec\n",
      "Epoch 19 Batch 600 Train Loss 0.1990 Time 44.70 sec\n",
      "Epoch 19 Batch 700 Train Loss 0.2612 Time 48.36 sec\n",
      "Epoch 19 Batch 800 Train Loss 0.2315 Time 51.84 sec\n",
      "Epoch 19 Batch 900 Train Loss 0.4127 Time 55.25 sec\n",
      "Epoch 19 Batch 1000 Train Loss 0.2088 Time 59.59 sec\n",
      "Epoch 19 Batch 1100 Train Loss 0.3375 Time 61.92 sec\n",
      "Epoch 19 Batch 1200 Train Loss 0.2916 Time 68.02 sec\n",
      "Epoch 19 Batch 1300 Train Loss 0.2703 Time 69.43 sec\n",
      "Epoch 19 Batch 1400 Train Loss 0.2156 Time 73.11 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Epoch 19 Test Loss 0.2841 \n",
      "\n",
      "**Epoch 19 Train Loss 0.2478\n",
      "Time taken for 1 epoch 824.50 sec\n",
      "\n",
      "Epoch 20 Batch 0 Train Loss 0.2187 Time 0.25 sec\n",
      "Epoch 20 Batch 100 Train Loss 0.2531 Time 28.54 sec\n",
      "Epoch 20 Batch 200 Train Loss 0.2077 Time 31.76 sec\n",
      "Epoch 20 Batch 300 Train Loss 0.1748 Time 35.22 sec\n",
      "Epoch 20 Batch 400 Train Loss 0.1851 Time 38.38 sec\n",
      "Epoch 20 Batch 500 Train Loss 0.2369 Time 41.68 sec\n",
      "Epoch 20 Batch 600 Train Loss 0.2117 Time 45.23 sec\n",
      "Epoch 20 Batch 700 Train Loss 0.2252 Time 48.82 sec\n",
      "Epoch 20 Batch 800 Train Loss 0.2333 Time 51.68 sec\n",
      "Epoch 20 Batch 900 Train Loss 0.2927 Time 55.36 sec\n",
      "Epoch 20 Batch 1000 Train Loss 0.2131 Time 58.20 sec\n",
      "Epoch 20 Batch 1100 Train Loss 0.2320 Time 61.75 sec\n",
      "Epoch 20 Batch 1200 Train Loss 0.2460 Time 65.35 sec\n",
      "Epoch 20 Batch 1300 Train Loss 0.2806 Time 69.25 sec\n",
      "Epoch 20 Batch 1400 Train Loss 0.2128 Time 72.99 sec\n",
      "\n",
      "**Epoch 20 Test Loss 0.2098 \n",
      "\n",
      "**Epoch 20 Train Loss 0.2278\n",
      "Time taken for 1 epoch 821.18 sec\n",
      "\n",
      "Epoch 21 Batch 0 Train Loss 0.1646 Time 0.27 sec\n",
      "Epoch 21 Batch 100 Train Loss 0.2399 Time 28.54 sec\n",
      "Epoch 21 Batch 200 Train Loss 0.1688 Time 32.04 sec\n",
      "Epoch 21 Batch 300 Train Loss 0.2406 Time 35.10 sec\n",
      "Epoch 21 Batch 400 Train Loss 0.2273 Time 38.23 sec\n",
      "Epoch 21 Batch 500 Train Loss 0.2324 Time 41.63 sec\n",
      "Epoch 21 Batch 600 Train Loss 0.2257 Time 45.70 sec\n",
      "Epoch 21 Batch 700 Train Loss 0.2143 Time 48.56 sec\n",
      "Epoch 21 Batch 800 Train Loss 0.1912 Time 51.50 sec\n",
      "Epoch 21 Batch 900 Train Loss 0.2323 Time 54.86 sec\n",
      "Epoch 21 Batch 1000 Train Loss 1.0477 Time 58.18 sec\n",
      "Epoch 21 Batch 1100 Train Loss 0.2298 Time 61.67 sec\n",
      "Epoch 21 Batch 1200 Train Loss 0.3784 Time 65.21 sec\n",
      "Epoch 21 Batch 1300 Train Loss 0.3749 Time 68.50 sec\n",
      "Epoch 21 Batch 1400 Train Loss 0.2950 Time 72.73 sec\n",
      "\n",
      "**Epoch 21 Test Loss 0.2601 \n",
      "\n",
      "**Epoch 21 Train Loss 0.2878\n",
      "Time taken for 1 epoch 819.35 sec\n",
      "\n",
      "Epoch 22 Batch 0 Train Loss 0.1959 Time 0.26 sec\n",
      "Epoch 22 Batch 100 Train Loss 0.2685 Time 28.28 sec\n",
      "Epoch 22 Batch 200 Train Loss 0.2146 Time 31.91 sec\n",
      "Epoch 22 Batch 300 Train Loss 0.1912 Time 34.97 sec\n",
      "Epoch 22 Batch 400 Train Loss 0.2293 Time 38.23 sec\n",
      "Epoch 22 Batch 500 Train Loss 0.2203 Time 41.75 sec\n",
      "Epoch 22 Batch 600 Train Loss 0.2388 Time 44.93 sec\n",
      "Epoch 22 Batch 700 Train Loss 0.2988 Time 48.57 sec\n",
      "Epoch 22 Batch 800 Train Loss 0.2280 Time 52.62 sec\n",
      "Epoch 22 Batch 900 Train Loss 0.2157 Time 55.41 sec\n",
      "Epoch 22 Batch 1000 Train Loss 0.1938 Time 58.25 sec\n",
      "Epoch 22 Batch 1100 Train Loss 0.1724 Time 62.12 sec\n",
      "Epoch 22 Batch 1200 Train Loss 0.2675 Time 65.47 sec\n",
      "Epoch 22 Batch 1300 Train Loss 0.2222 Time 69.36 sec\n",
      "Epoch 22 Batch 1400 Train Loss 0.2077 Time 73.14 sec\n",
      "\n",
      "**Epoch 22 Test Loss 0.2106 \n",
      "\n",
      "**Epoch 22 Train Loss 0.2225\n",
      "Time taken for 1 epoch 822.33 sec\n",
      "\n",
      "Epoch 23 Batch 0 Train Loss 0.1420 Time 0.26 sec\n",
      "Epoch 23 Batch 100 Train Loss 0.2211 Time 28.32 sec\n",
      "Epoch 23 Batch 200 Train Loss 0.2090 Time 31.62 sec\n",
      "Epoch 23 Batch 300 Train Loss 0.1886 Time 34.83 sec\n",
      "Epoch 23 Batch 400 Train Loss 0.2207 Time 38.38 sec\n",
      "Epoch 23 Batch 500 Train Loss 0.1853 Time 41.76 sec\n",
      "Epoch 23 Batch 600 Train Loss 0.2044 Time 46.07 sec\n",
      "Epoch 23 Batch 700 Train Loss 0.1791 Time 48.99 sec\n",
      "Epoch 23 Batch 800 Train Loss 0.2172 Time 52.21 sec\n",
      "Epoch 23 Batch 900 Train Loss 0.2286 Time 55.78 sec\n",
      "Epoch 23 Batch 1000 Train Loss 0.2317 Time 58.50 sec\n",
      "Epoch 23 Batch 1100 Train Loss 0.1982 Time 61.72 sec\n",
      "Epoch 23 Batch 1200 Train Loss 0.2939 Time 65.93 sec\n",
      "Epoch 23 Batch 1300 Train Loss 0.2463 Time 69.42 sec\n",
      "Epoch 23 Batch 1400 Train Loss 0.2510 Time 73.50 sec\n",
      "\n",
      "**Epoch 23 Test Loss 0.2019 \n",
      "\n",
      "**Epoch 23 Train Loss 0.2088\n",
      "Time taken for 1 epoch 825.22 sec\n",
      "\n",
      "Epoch 24 Batch 0 Train Loss 0.2001 Time 0.30 sec\n",
      "Epoch 24 Batch 100 Train Loss 0.1772 Time 30.72 sec\n",
      "Epoch 24 Batch 200 Train Loss 0.2011 Time 31.72 sec\n",
      "Epoch 24 Batch 300 Train Loss 0.1519 Time 35.23 sec\n",
      "Epoch 24 Batch 400 Train Loss 0.2386 Time 38.93 sec\n",
      "Epoch 24 Batch 500 Train Loss 0.1629 Time 41.60 sec\n",
      "Epoch 24 Batch 600 Train Loss 0.1765 Time 45.37 sec\n",
      "Epoch 24 Batch 700 Train Loss 0.2329 Time 48.67 sec\n",
      "Epoch 24 Batch 800 Train Loss 0.2009 Time 52.31 sec\n",
      "Epoch 24 Batch 900 Train Loss 0.3257 Time 55.86 sec\n",
      "Epoch 24 Batch 1000 Train Loss 0.3395 Time 58.66 sec\n",
      "Epoch 24 Batch 1100 Train Loss 0.2754 Time 62.15 sec\n",
      "Epoch 24 Batch 1200 Train Loss 0.2355 Time 65.95 sec\n",
      "Epoch 24 Batch 1300 Train Loss 1.0789 Time 69.16 sec\n",
      "Epoch 24 Batch 1400 Train Loss 0.2754 Time 73.53 sec\n",
      "\n",
      "**Epoch 24 Test Loss 0.2609 \n",
      "\n",
      "**Epoch 24 Train Loss 0.2550\n",
      "Time taken for 1 epoch 827.04 sec\n",
      "\n",
      "Epoch 25 Batch 0 Train Loss 0.2864 Time 0.30 sec\n",
      "Epoch 25 Batch 100 Train Loss 0.2604 Time 28.59 sec\n",
      "Epoch 25 Batch 200 Train Loss 0.2088 Time 31.66 sec\n",
      "Epoch 25 Batch 300 Train Loss 0.2268 Time 35.10 sec\n",
      "Epoch 25 Batch 400 Train Loss 0.3324 Time 38.64 sec\n",
      "Epoch 25 Batch 500 Train Loss 0.2593 Time 41.59 sec\n",
      "Epoch 25 Batch 600 Train Loss 0.2052 Time 44.97 sec\n",
      "Epoch 25 Batch 700 Train Loss 0.2024 Time 48.68 sec\n",
      "Epoch 25 Batch 800 Train Loss 0.2408 Time 52.22 sec\n",
      "Epoch 25 Batch 900 Train Loss 0.2363 Time 55.69 sec\n",
      "Epoch 25 Batch 1000 Train Loss 0.2259 Time 59.06 sec\n",
      "Epoch 25 Batch 1100 Train Loss 0.2037 Time 62.08 sec\n",
      "Epoch 25 Batch 1200 Train Loss 0.2835 Time 65.84 sec\n",
      "Epoch 25 Batch 1300 Train Loss 0.2114 Time 69.44 sec\n",
      "Epoch 25 Batch 1400 Train Loss 0.2153 Time 73.31 sec\n",
      "\n",
      "**Epoch 25 Test Loss 0.2282 \n",
      "\n",
      "**Epoch 25 Train Loss 0.2220\n",
      "Time taken for 1 epoch 824.81 sec\n",
      "\n",
      "Epoch 26 Batch 0 Train Loss 0.2011 Time 0.27 sec\n",
      "Epoch 26 Batch 100 Train Loss 0.2120 Time 28.75 sec\n",
      "Epoch 26 Batch 200 Train Loss 0.1863 Time 31.71 sec\n",
      "Epoch 26 Batch 300 Train Loss 0.1906 Time 35.26 sec\n",
      "Epoch 26 Batch 400 Train Loss 0.1958 Time 38.06 sec\n",
      "Epoch 26 Batch 500 Train Loss 0.2307 Time 41.58 sec\n",
      "Epoch 26 Batch 600 Train Loss 0.2336 Time 45.29 sec\n",
      "Epoch 26 Batch 700 Train Loss 0.2189 Time 49.34 sec\n",
      "Epoch 26 Batch 800 Train Loss 0.2004 Time 52.07 sec\n",
      "Epoch 26 Batch 900 Train Loss 0.2261 Time 55.24 sec\n",
      "Epoch 26 Batch 1000 Train Loss 0.2026 Time 58.78 sec\n",
      "Epoch 26 Batch 1100 Train Loss 0.1989 Time 61.65 sec\n",
      "Epoch 26 Batch 1200 Train Loss 0.2545 Time 65.91 sec\n",
      "Epoch 26 Batch 1300 Train Loss 0.2172 Time 69.24 sec\n",
      "Epoch 26 Batch 1400 Train Loss 0.2565 Time 72.81 sec\n",
      "\n",
      "**Epoch 26 Test Loss 0.2940 \n",
      "\n",
      "**Epoch 26 Train Loss 0.2306\n",
      "Time taken for 1 epoch 823.09 sec\n",
      "\n",
      "Epoch 27 Batch 0 Train Loss 0.1880 Time 0.31 sec\n",
      "Epoch 27 Batch 100 Train Loss 0.2284 Time 28.50 sec\n",
      "Epoch 27 Batch 200 Train Loss 0.2022 Time 31.61 sec\n",
      "Epoch 27 Batch 300 Train Loss 0.3027 Time 35.08 sec\n",
      "Epoch 27 Batch 400 Train Loss 0.2200 Time 38.24 sec\n",
      "Epoch 27 Batch 500 Train Loss 0.1702 Time 41.28 sec\n",
      "Epoch 27 Batch 600 Train Loss 0.1788 Time 45.05 sec\n",
      "Epoch 27 Batch 700 Train Loss 0.2253 Time 48.58 sec\n",
      "Epoch 27 Batch 800 Train Loss 0.1753 Time 51.81 sec\n",
      "Epoch 27 Batch 900 Train Loss 0.2018 Time 55.29 sec\n",
      "Epoch 27 Batch 1000 Train Loss 0.2016 Time 58.74 sec\n",
      "Epoch 27 Batch 1100 Train Loss 0.1963 Time 62.47 sec\n",
      "Epoch 27 Batch 1200 Train Loss 0.1996 Time 66.24 sec\n",
      "Epoch 27 Batch 1300 Train Loss 0.2229 Time 69.50 sec\n",
      "Epoch 27 Batch 1400 Train Loss 0.2469 Time 73.46 sec\n",
      "\n",
      "**Epoch 27 Test Loss 0.2030 \n",
      "\n",
      "**Epoch 27 Train Loss 0.2149\n",
      "Time taken for 1 epoch 823.28 sec\n",
      "\n",
      "Epoch 28 Batch 0 Train Loss 0.1388 Time 0.26 sec\n",
      "Epoch 28 Batch 100 Train Loss 0.2057 Time 28.49 sec\n",
      "Epoch 28 Batch 200 Train Loss 0.1983 Time 31.61 sec\n",
      "Epoch 28 Batch 300 Train Loss 0.1382 Time 34.98 sec\n",
      "Epoch 28 Batch 400 Train Loss 0.1748 Time 38.70 sec\n",
      "Epoch 28 Batch 500 Train Loss 0.1919 Time 41.97 sec\n",
      "Epoch 28 Batch 600 Train Loss 0.1589 Time 45.50 sec\n",
      "Epoch 28 Batch 700 Train Loss 0.1930 Time 48.50 sec\n",
      "Epoch 28 Batch 800 Train Loss 0.2056 Time 53.89 sec\n",
      "Epoch 28 Batch 900 Train Loss 0.2427 Time 56.05 sec\n",
      "Epoch 28 Batch 1000 Train Loss 0.1719 Time 59.22 sec\n",
      "Epoch 28 Batch 1100 Train Loss 0.2447 Time 62.14 sec\n",
      "Epoch 28 Batch 1200 Train Loss 0.2055 Time 65.71 sec\n",
      "Epoch 28 Batch 1300 Train Loss 0.1997 Time 69.43 sec\n",
      "Epoch 28 Batch 1400 Train Loss 0.2171 Time 73.56 sec\n",
      "\n",
      "**Epoch 28 Test Loss 0.1904 \n",
      "\n",
      "**Epoch 28 Train Loss 0.1926\n",
      "Time taken for 1 epoch 827.82 sec\n",
      "\n",
      "Epoch 29 Batch 0 Train Loss 0.1917 Time 0.27 sec\n",
      "Epoch 29 Batch 100 Train Loss 0.1823 Time 28.46 sec\n",
      "Epoch 29 Batch 200 Train Loss 0.1754 Time 31.79 sec\n",
      "Epoch 29 Batch 300 Train Loss 0.2031 Time 35.47 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 400 Train Loss 0.1981 Time 38.37 sec\n",
      "Epoch 29 Batch 500 Train Loss 0.1941 Time 41.54 sec\n",
      "Epoch 29 Batch 600 Train Loss 0.1920 Time 45.41 sec\n",
      "Epoch 29 Batch 700 Train Loss 0.2202 Time 49.04 sec\n",
      "Epoch 29 Batch 800 Train Loss 0.1874 Time 52.19 sec\n",
      "Epoch 29 Batch 900 Train Loss 0.1976 Time 55.84 sec\n",
      "Epoch 29 Batch 1000 Train Loss 0.1605 Time 58.63 sec\n",
      "Epoch 29 Batch 1100 Train Loss 0.1763 Time 62.35 sec\n",
      "Epoch 29 Batch 1200 Train Loss 0.1610 Time 66.02 sec\n",
      "Epoch 29 Batch 1300 Train Loss 0.2147 Time 69.58 sec\n",
      "Epoch 29 Batch 1400 Train Loss 0.1877 Time 73.39 sec\n",
      "\n",
      "**Epoch 29 Test Loss 0.1778 \n",
      "\n",
      "**Epoch 29 Train Loss 0.1865\n",
      "Time taken for 1 epoch 826.61 sec\n",
      "\n",
      "Epoch 30 Batch 0 Train Loss 0.1771 Time 0.27 sec\n",
      "Epoch 30 Batch 100 Train Loss 0.2242 Time 28.63 sec\n",
      "Epoch 30 Batch 200 Train Loss 0.1342 Time 31.87 sec\n",
      "Epoch 30 Batch 300 Train Loss 0.1589 Time 35.20 sec\n",
      "Epoch 30 Batch 400 Train Loss 0.1699 Time 38.45 sec\n",
      "Epoch 30 Batch 500 Train Loss 0.1925 Time 41.58 sec\n",
      "Epoch 30 Batch 600 Train Loss 0.1676 Time 45.32 sec\n",
      "Epoch 30 Batch 700 Train Loss 0.1833 Time 48.62 sec\n",
      "Epoch 30 Batch 800 Train Loss 0.1868 Time 52.13 sec\n",
      "Epoch 30 Batch 900 Train Loss 0.1845 Time 55.93 sec\n",
      "Epoch 30 Batch 1000 Train Loss 0.1916 Time 59.12 sec\n",
      "Epoch 30 Batch 1100 Train Loss 0.2086 Time 62.11 sec\n",
      "Epoch 30 Batch 1200 Train Loss 0.4134 Time 65.39 sec\n",
      "Epoch 30 Batch 1300 Train Loss 0.2411 Time 69.26 sec\n",
      "Epoch 30 Batch 1400 Train Loss 0.2688 Time 73.25 sec\n",
      "\n",
      "**Epoch 30 Test Loss 0.1953 \n",
      "\n",
      "**Epoch 30 Train Loss 0.1940\n",
      "Time taken for 1 epoch 824.83 sec\n",
      "\n",
      "Epoch 31 Batch 0 Train Loss 0.1476 Time 0.26 sec\n",
      "Epoch 31 Batch 100 Train Loss 0.1832 Time 28.74 sec\n",
      "Epoch 31 Batch 200 Train Loss 0.2025 Time 31.83 sec\n",
      "Epoch 31 Batch 300 Train Loss 0.1543 Time 35.06 sec\n",
      "Epoch 31 Batch 400 Train Loss 0.1738 Time 38.29 sec\n",
      "Epoch 31 Batch 500 Train Loss 0.2701 Time 41.72 sec\n",
      "Epoch 31 Batch 600 Train Loss 0.2090 Time 45.33 sec\n",
      "Epoch 31 Batch 700 Train Loss 0.1765 Time 49.05 sec\n",
      "Epoch 31 Batch 800 Train Loss 0.2083 Time 52.43 sec\n",
      "Epoch 31 Batch 900 Train Loss 0.1856 Time 55.51 sec\n",
      "Epoch 31 Batch 1000 Train Loss 0.1990 Time 59.51 sec\n",
      "Epoch 31 Batch 1100 Train Loss 0.1624 Time 62.38 sec\n",
      "Epoch 31 Batch 1200 Train Loss 0.1670 Time 65.82 sec\n",
      "Epoch 31 Batch 1300 Train Loss 0.1749 Time 69.53 sec\n",
      "Epoch 31 Batch 1400 Train Loss 0.2319 Time 73.78 sec\n",
      "\n",
      "**Epoch 31 Test Loss 0.1893 \n",
      "\n",
      "**Epoch 31 Train Loss 0.1883\n",
      "Time taken for 1 epoch 828.02 sec\n",
      "\n",
      "Epoch 32 Batch 0 Train Loss 0.1979 Time 0.27 sec\n",
      "Epoch 32 Batch 100 Train Loss 0.2378 Time 28.57 sec\n",
      "Epoch 32 Batch 200 Train Loss 0.1775 Time 31.98 sec\n",
      "Epoch 32 Batch 300 Train Loss 0.1975 Time 34.97 sec\n",
      "Epoch 32 Batch 400 Train Loss 0.1540 Time 38.39 sec\n",
      "Epoch 32 Batch 500 Train Loss 0.1622 Time 41.54 sec\n",
      "Epoch 32 Batch 600 Train Loss 0.1677 Time 45.13 sec\n",
      "Epoch 32 Batch 700 Train Loss 0.1773 Time 49.16 sec\n",
      "Epoch 32 Batch 800 Train Loss 0.1620 Time 51.93 sec\n",
      "Epoch 32 Batch 900 Train Loss 0.1921 Time 55.29 sec\n",
      "Epoch 32 Batch 1000 Train Loss 0.1678 Time 58.72 sec\n",
      "Epoch 32 Batch 1100 Train Loss 0.1533 Time 63.58 sec\n",
      "Epoch 32 Batch 1200 Train Loss 0.1922 Time 65.94 sec\n",
      "Epoch 32 Batch 1300 Train Loss 0.1684 Time 71.72 sec\n",
      "Epoch 32 Batch 1400 Train Loss 0.1478 Time 73.73 sec\n",
      "\n",
      "**Epoch 32 Test Loss 0.1732 \n",
      "\n",
      "**Epoch 32 Train Loss 0.1778\n",
      "Time taken for 1 epoch 828.76 sec\n",
      "\n",
      "Epoch 33 Batch 0 Train Loss 0.1848 Time 0.26 sec\n",
      "Epoch 33 Batch 100 Train Loss 0.1266 Time 28.54 sec\n",
      "Epoch 33 Batch 200 Train Loss 0.1191 Time 31.66 sec\n",
      "Epoch 33 Batch 300 Train Loss 0.1530 Time 35.24 sec\n",
      "Epoch 33 Batch 400 Train Loss 0.1616 Time 38.51 sec\n",
      "Epoch 33 Batch 500 Train Loss 0.1547 Time 41.55 sec\n",
      "Epoch 33 Batch 600 Train Loss 0.1378 Time 45.24 sec\n",
      "Epoch 33 Batch 700 Train Loss 0.1770 Time 48.85 sec\n",
      "Epoch 33 Batch 800 Train Loss 0.1490 Time 52.00 sec\n",
      "Epoch 33 Batch 900 Train Loss 0.2703 Time 55.33 sec\n",
      "Epoch 33 Batch 1000 Train Loss 0.1446 Time 58.17 sec\n",
      "Epoch 33 Batch 1100 Train Loss 0.1684 Time 62.03 sec\n",
      "Epoch 33 Batch 1200 Train Loss 0.1728 Time 65.52 sec\n",
      "Epoch 33 Batch 1300 Train Loss 0.1724 Time 69.10 sec\n",
      "Epoch 33 Batch 1400 Train Loss 0.1877 Time 72.90 sec\n",
      "\n",
      "**Epoch 33 Test Loss 0.1642 \n",
      "\n",
      "**Epoch 33 Train Loss 0.1669\n",
      "Time taken for 1 epoch 821.57 sec\n",
      "\n",
      "Epoch 34 Batch 0 Train Loss 0.1566 Time 0.30 sec\n",
      "Epoch 34 Batch 100 Train Loss 0.1151 Time 28.48 sec\n",
      "Epoch 34 Batch 200 Train Loss 0.1575 Time 31.64 sec\n",
      "Epoch 34 Batch 300 Train Loss 0.1564 Time 35.02 sec\n",
      "Epoch 34 Batch 400 Train Loss 0.2069 Time 38.31 sec\n",
      "Epoch 34 Batch 500 Train Loss 0.1664 Time 41.70 sec\n",
      "Epoch 34 Batch 600 Train Loss 0.1618 Time 44.85 sec\n",
      "Epoch 34 Batch 700 Train Loss 0.1997 Time 48.67 sec\n",
      "Epoch 34 Batch 800 Train Loss 0.1672 Time 52.16 sec\n",
      "Epoch 34 Batch 900 Train Loss 0.1642 Time 55.28 sec\n",
      "Epoch 34 Batch 1000 Train Loss 0.1731 Time 58.46 sec\n",
      "Epoch 34 Batch 1100 Train Loss 0.1690 Time 61.87 sec\n",
      "Epoch 34 Batch 1200 Train Loss 0.1754 Time 65.32 sec\n",
      "Epoch 34 Batch 1300 Train Loss 0.2054 Time 69.14 sec\n",
      "Epoch 34 Batch 1400 Train Loss 0.1709 Time 73.20 sec\n",
      "\n",
      "**Epoch 34 Test Loss 0.1640 \n",
      "\n",
      "**Epoch 34 Train Loss 0.1690\n",
      "Time taken for 1 epoch 821.44 sec\n",
      "\n",
      "Epoch 35 Batch 0 Train Loss 0.1359 Time 0.27 sec\n",
      "Epoch 35 Batch 100 Train Loss 0.1317 Time 28.45 sec\n",
      "Epoch 35 Batch 200 Train Loss 0.1454 Time 31.83 sec\n",
      "Epoch 35 Batch 300 Train Loss 0.1318 Time 35.04 sec\n",
      "Epoch 35 Batch 400 Train Loss 0.1529 Time 38.40 sec\n",
      "Epoch 35 Batch 500 Train Loss 0.1338 Time 41.55 sec\n",
      "Epoch 35 Batch 600 Train Loss 0.1573 Time 45.07 sec\n",
      "Epoch 35 Batch 700 Train Loss 0.1723 Time 48.84 sec\n",
      "Epoch 35 Batch 800 Train Loss 0.1474 Time 52.04 sec\n",
      "Epoch 35 Batch 900 Train Loss 0.1357 Time 55.31 sec\n",
      "Epoch 35 Batch 1000 Train Loss 0.1431 Time 58.97 sec\n",
      "Epoch 35 Batch 1100 Train Loss 0.2720 Time 62.08 sec\n",
      "Epoch 35 Batch 1200 Train Loss 0.2032 Time 65.79 sec\n",
      "Epoch 35 Batch 1300 Train Loss 0.1970 Time 69.11 sec\n",
      "Epoch 35 Batch 1400 Train Loss 0.1688 Time 73.13 sec\n",
      "\n",
      "**Epoch 35 Test Loss 0.1762 \n",
      "\n",
      "**Epoch 35 Train Loss 0.1763\n",
      "Time taken for 1 epoch 823.17 sec\n",
      "\n",
      "Epoch 36 Batch 0 Train Loss 0.1379 Time 0.30 sec\n",
      "Epoch 36 Batch 100 Train Loss 0.1815 Time 28.39 sec\n",
      "Epoch 36 Batch 200 Train Loss 0.1658 Time 31.66 sec\n",
      "Epoch 36 Batch 300 Train Loss 0.1772 Time 35.24 sec\n",
      "Epoch 36 Batch 400 Train Loss 0.1883 Time 38.36 sec\n",
      "Epoch 36 Batch 500 Train Loss 0.1621 Time 41.53 sec\n",
      "Epoch 36 Batch 600 Train Loss 0.1795 Time 45.25 sec\n",
      "Epoch 36 Batch 700 Train Loss 0.2033 Time 48.34 sec\n",
      "Epoch 36 Batch 800 Train Loss 0.1908 Time 52.63 sec\n",
      "Epoch 36 Batch 900 Train Loss 0.1393 Time 55.52 sec\n",
      "Epoch 36 Batch 1000 Train Loss 0.1181 Time 58.48 sec\n",
      "Epoch 36 Batch 1100 Train Loss 0.1531 Time 61.85 sec\n",
      "Epoch 36 Batch 1200 Train Loss 0.1580 Time 65.26 sec\n",
      "Epoch 36 Batch 1300 Train Loss 0.1510 Time 69.24 sec\n",
      "Epoch 36 Batch 1400 Train Loss 0.1749 Time 73.16 sec\n",
      "\n",
      "**Epoch 36 Test Loss 0.1555 \n",
      "\n",
      "**Epoch 36 Train Loss 0.1601\n",
      "Time taken for 1 epoch 823.07 sec\n",
      "\n",
      "Epoch 37 Batch 0 Train Loss 0.1380 Time 0.26 sec\n",
      "Epoch 37 Batch 100 Train Loss 0.1620 Time 28.31 sec\n",
      "Epoch 37 Batch 200 Train Loss 0.1423 Time 31.68 sec\n",
      "Epoch 37 Batch 300 Train Loss 0.1805 Time 36.82 sec\n",
      "Epoch 37 Batch 400 Train Loss 0.1696 Time 39.09 sec\n",
      "Epoch 37 Batch 500 Train Loss 0.1622 Time 41.73 sec\n",
      "Epoch 37 Batch 600 Train Loss 0.1587 Time 45.48 sec\n",
      "Epoch 37 Batch 700 Train Loss 0.1658 Time 48.82 sec\n",
      "Epoch 37 Batch 800 Train Loss 0.1586 Time 52.34 sec\n",
      "Epoch 37 Batch 900 Train Loss 0.1437 Time 55.48 sec\n",
      "Epoch 37 Batch 1000 Train Loss 0.1484 Time 58.88 sec\n",
      "Epoch 37 Batch 1100 Train Loss 0.2960 Time 62.32 sec\n",
      "Epoch 37 Batch 1200 Train Loss 0.2525 Time 66.15 sec\n",
      "Epoch 37 Batch 1300 Train Loss 0.1788 Time 69.18 sec\n",
      "Epoch 37 Batch 1400 Train Loss 0.1479 Time 73.38 sec\n",
      "\n",
      "**Epoch 37 Test Loss 0.1602 \n",
      "\n",
      "**Epoch 37 Train Loss 0.1632\n",
      "Time taken for 1 epoch 827.44 sec\n",
      "\n",
      "Epoch 38 Batch 0 Train Loss 0.1591 Time 0.27 sec\n",
      "Epoch 38 Batch 100 Train Loss 0.1672 Time 28.64 sec\n",
      "Epoch 38 Batch 200 Train Loss 0.1422 Time 31.79 sec\n",
      "Epoch 38 Batch 300 Train Loss 0.1356 Time 35.25 sec\n",
      "Epoch 38 Batch 400 Train Loss 0.1987 Time 38.45 sec\n",
      "Epoch 38 Batch 500 Train Loss 0.1597 Time 41.62 sec\n",
      "Epoch 38 Batch 600 Train Loss 0.1606 Time 45.24 sec\n",
      "Epoch 38 Batch 700 Train Loss 0.2090 Time 48.89 sec\n",
      "Epoch 38 Batch 800 Train Loss 0.2171 Time 52.29 sec\n",
      "Epoch 38 Batch 900 Train Loss 0.1669 Time 55.62 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 1000 Train Loss 0.1542 Time 58.75 sec\n",
      "Epoch 38 Batch 1100 Train Loss 0.1512 Time 61.87 sec\n",
      "Epoch 38 Batch 1200 Train Loss 0.1774 Time 66.27 sec\n",
      "Epoch 38 Batch 1300 Train Loss 0.1569 Time 69.97 sec\n",
      "Epoch 38 Batch 1400 Train Loss 0.1927 Time 73.22 sec\n",
      "\n",
      "**Epoch 38 Test Loss 0.1533 \n",
      "\n",
      "**Epoch 38 Train Loss 0.1594\n",
      "Time taken for 1 epoch 825.85 sec\n",
      "\n",
      "Epoch 39 Batch 0 Train Loss 0.1003 Time 0.27 sec\n",
      "Epoch 39 Batch 100 Train Loss 0.1989 Time 28.63 sec\n",
      "Epoch 39 Batch 200 Train Loss 0.1614 Time 31.79 sec\n",
      "Epoch 39 Batch 300 Train Loss 0.1288 Time 35.11 sec\n",
      "Epoch 39 Batch 400 Train Loss 0.1504 Time 38.22 sec\n",
      "Epoch 39 Batch 500 Train Loss 0.1053 Time 41.63 sec\n",
      "Epoch 39 Batch 600 Train Loss 0.1493 Time 46.11 sec\n",
      "Epoch 39 Batch 700 Train Loss 0.1657 Time 48.76 sec\n",
      "Epoch 39 Batch 800 Train Loss 0.1639 Time 52.15 sec\n",
      "Epoch 39 Batch 900 Train Loss 0.1460 Time 55.67 sec\n",
      "Epoch 39 Batch 1000 Train Loss 0.1242 Time 60.12 sec\n",
      "Epoch 39 Batch 1100 Train Loss 0.1608 Time 62.41 sec\n",
      "Epoch 39 Batch 1200 Train Loss 0.1519 Time 66.08 sec\n",
      "Epoch 39 Batch 1300 Train Loss 0.1505 Time 69.41 sec\n",
      "Epoch 39 Batch 1400 Train Loss 0.2627 Time 73.98 sec\n",
      "\n",
      "**Epoch 39 Test Loss 0.2831 \n",
      "\n",
      "**Epoch 39 Train Loss 0.1690\n",
      "Time taken for 1 epoch 828.14 sec\n",
      "\n",
      "Epoch 40 Batch 0 Train Loss 0.2605 Time 0.27 sec\n",
      "Epoch 40 Batch 100 Train Loss 0.2433 Time 28.64 sec\n",
      "Epoch 40 Batch 200 Train Loss 0.2141 Time 31.97 sec\n",
      "Epoch 40 Batch 300 Train Loss 0.2242 Time 35.01 sec\n",
      "Epoch 40 Batch 400 Train Loss 0.2166 Time 38.65 sec\n",
      "Epoch 40 Batch 500 Train Loss 0.1622 Time 42.07 sec\n",
      "Epoch 40 Batch 600 Train Loss 0.1755 Time 45.29 sec\n",
      "Epoch 40 Batch 700 Train Loss 0.2099 Time 49.09 sec\n",
      "Epoch 40 Batch 800 Train Loss 0.1771 Time 52.63 sec\n",
      "Epoch 40 Batch 900 Train Loss 0.1760 Time 55.61 sec\n",
      "Epoch 40 Batch 1000 Train Loss 0.1823 Time 58.84 sec\n",
      "Epoch 40 Batch 1100 Train Loss 0.1888 Time 62.15 sec\n",
      "Epoch 40 Batch 1200 Train Loss 0.2119 Time 66.12 sec\n",
      "Epoch 40 Batch 1300 Train Loss 0.1846 Time 70.16 sec\n",
      "Epoch 40 Batch 1400 Train Loss 0.1736 Time 73.53 sec\n",
      "\n",
      "**Epoch 40 Test Loss 0.1730 \n",
      "\n",
      "**Epoch 40 Train Loss 0.2013\n",
      "Time taken for 1 epoch 827.89 sec\n",
      "\n",
      "Epoch 41 Batch 0 Train Loss 0.1218 Time 0.27 sec\n",
      "Epoch 41 Batch 100 Train Loss 0.1971 Time 28.88 sec\n",
      "Epoch 41 Batch 200 Train Loss 0.1404 Time 31.72 sec\n",
      "Epoch 41 Batch 300 Train Loss 0.1405 Time 35.17 sec\n",
      "Epoch 41 Batch 400 Train Loss 0.1615 Time 38.52 sec\n",
      "Epoch 41 Batch 500 Train Loss 0.1439 Time 41.44 sec\n",
      "Epoch 41 Batch 600 Train Loss 0.1717 Time 44.92 sec\n",
      "Epoch 41 Batch 700 Train Loss 0.1405 Time 49.00 sec\n",
      "Epoch 41 Batch 800 Train Loss 0.1999 Time 51.93 sec\n",
      "Epoch 41 Batch 900 Train Loss 0.1668 Time 55.97 sec\n",
      "Epoch 41 Batch 1000 Train Loss 0.1832 Time 60.68 sec\n",
      "Epoch 41 Batch 1100 Train Loss 0.1518 Time 62.09 sec\n",
      "Epoch 41 Batch 1200 Train Loss 0.1606 Time 66.10 sec\n",
      "Epoch 41 Batch 1300 Train Loss 0.1569 Time 69.43 sec\n",
      "Epoch 41 Batch 1400 Train Loss 0.1500 Time 73.37 sec\n",
      "\n",
      "**Epoch 41 Test Loss 0.1571 \n",
      "\n",
      "**Epoch 41 Train Loss 0.1612\n",
      "Time taken for 1 epoch 826.48 sec\n",
      "\n",
      "Epoch 42 Batch 0 Train Loss 0.1535 Time 0.27 sec\n",
      "Epoch 42 Batch 100 Train Loss 0.1772 Time 28.67 sec\n",
      "Epoch 42 Batch 200 Train Loss 0.1329 Time 31.71 sec\n",
      "Epoch 42 Batch 300 Train Loss 0.1510 Time 35.15 sec\n",
      "Epoch 42 Batch 400 Train Loss 0.1723 Time 38.17 sec\n",
      "Epoch 42 Batch 500 Train Loss 0.1214 Time 41.63 sec\n",
      "Epoch 42 Batch 600 Train Loss 0.1494 Time 45.39 sec\n",
      "Epoch 42 Batch 700 Train Loss 0.1640 Time 48.74 sec\n",
      "Epoch 42 Batch 800 Train Loss 0.1415 Time 52.03 sec\n",
      "Epoch 42 Batch 900 Train Loss 0.1734 Time 55.64 sec\n",
      "Epoch 42 Batch 1000 Train Loss 0.1369 Time 58.69 sec\n",
      "Epoch 42 Batch 1100 Train Loss 0.1815 Time 62.05 sec\n",
      "Epoch 42 Batch 1200 Train Loss 0.1456 Time 65.79 sec\n",
      "Epoch 42 Batch 1300 Train Loss 0.1770 Time 69.14 sec\n",
      "Epoch 42 Batch 1400 Train Loss 0.1480 Time 73.03 sec\n",
      "\n",
      "**Epoch 42 Test Loss 0.1484 \n",
      "\n",
      "**Epoch 42 Train Loss 0.1508\n",
      "Time taken for 1 epoch 824.16 sec\n",
      "\n",
      "Epoch 43 Batch 0 Train Loss 0.1247 Time 0.27 sec\n",
      "Epoch 43 Batch 100 Train Loss 0.1721 Time 28.83 sec\n",
      "Epoch 43 Batch 200 Train Loss 0.1375 Time 31.99 sec\n",
      "Epoch 43 Batch 300 Train Loss 0.1428 Time 35.05 sec\n",
      "Epoch 43 Batch 400 Train Loss 0.1761 Time 38.45 sec\n",
      "Epoch 43 Batch 500 Train Loss 0.1362 Time 41.83 sec\n",
      "Epoch 43 Batch 600 Train Loss 0.1189 Time 44.98 sec\n",
      "Epoch 43 Batch 700 Train Loss 0.1340 Time 48.91 sec\n",
      "Epoch 43 Batch 800 Train Loss 0.1517 Time 52.47 sec\n",
      "Epoch 43 Batch 900 Train Loss 0.1431 Time 55.63 sec\n",
      "Epoch 43 Batch 1000 Train Loss 0.1406 Time 58.85 sec\n",
      "Epoch 43 Batch 1100 Train Loss 0.1468 Time 62.23 sec\n",
      "Epoch 43 Batch 1200 Train Loss 0.1638 Time 65.84 sec\n",
      "Epoch 43 Batch 1300 Train Loss 0.1413 Time 69.61 sec\n",
      "Epoch 43 Batch 1400 Train Loss 0.1751 Time 73.27 sec\n",
      "\n",
      "**Epoch 43 Test Loss 0.1503 \n",
      "\n",
      "**Epoch 43 Train Loss 0.1463\n",
      "Time taken for 1 epoch 826.23 sec\n",
      "\n",
      "Epoch 44 Batch 0 Train Loss 0.1334 Time 0.27 sec\n",
      "Epoch 44 Batch 100 Train Loss 0.1362 Time 28.49 sec\n",
      "Epoch 44 Batch 200 Train Loss 0.1245 Time 32.05 sec\n",
      "Epoch 44 Batch 300 Train Loss 0.1163 Time 35.01 sec\n",
      "Epoch 44 Batch 400 Train Loss 0.1383 Time 38.39 sec\n",
      "Epoch 44 Batch 500 Train Loss 0.1633 Time 41.72 sec\n",
      "Epoch 44 Batch 600 Train Loss 0.1225 Time 45.35 sec\n",
      "Epoch 44 Batch 700 Train Loss 0.1508 Time 48.99 sec\n",
      "Epoch 44 Batch 800 Train Loss 0.1777 Time 52.49 sec\n",
      "Epoch 44 Batch 900 Train Loss 0.1325 Time 56.24 sec\n",
      "Epoch 44 Batch 1000 Train Loss 0.1412 Time 59.08 sec\n",
      "Epoch 44 Batch 1100 Train Loss 0.1477 Time 62.01 sec\n",
      "Epoch 44 Batch 1200 Train Loss 0.1562 Time 65.79 sec\n",
      "Epoch 44 Batch 1300 Train Loss 0.3041 Time 69.61 sec\n",
      "Epoch 44 Batch 1400 Train Loss 0.1779 Time 73.66 sec\n",
      "\n",
      "**Epoch 44 Test Loss 0.1801 \n",
      "\n",
      "**Epoch 44 Train Loss 0.1630\n",
      "Time taken for 1 epoch 827.15 sec\n",
      "\n",
      "Epoch 45 Batch 0 Train Loss 0.1589 Time 0.28 sec\n",
      "Epoch 45 Batch 100 Train Loss 0.1578 Time 28.49 sec\n",
      "Epoch 45 Batch 200 Train Loss 0.1634 Time 32.23 sec\n",
      "Epoch 45 Batch 300 Train Loss 0.1222 Time 35.54 sec\n",
      "Epoch 45 Batch 400 Train Loss 0.1732 Time 38.49 sec\n",
      "Epoch 45 Batch 500 Train Loss 0.1134 Time 41.58 sec\n",
      "Epoch 45 Batch 600 Train Loss 0.1454 Time 45.60 sec\n",
      "Epoch 45 Batch 700 Train Loss 0.1870 Time 49.04 sec\n",
      "Epoch 45 Batch 800 Train Loss 0.1430 Time 52.08 sec\n",
      "Epoch 45 Batch 900 Train Loss 0.1531 Time 55.70 sec\n",
      "Epoch 45 Batch 1000 Train Loss 0.1464 Time 58.72 sec\n",
      "Epoch 45 Batch 1100 Train Loss 0.1890 Time 62.09 sec\n",
      "Epoch 45 Batch 1200 Train Loss 0.1368 Time 67.10 sec\n",
      "Epoch 45 Batch 1300 Train Loss 0.1541 Time 69.48 sec\n",
      "Epoch 45 Batch 1400 Train Loss 0.1452 Time 75.65 sec\n",
      "\n",
      "**Epoch 45 Test Loss 0.1475 \n",
      "\n",
      "**Epoch 45 Train Loss 0.1534\n",
      "Time taken for 1 epoch 830.06 sec\n",
      "\n",
      "Epoch 46 Batch 0 Train Loss 0.1149 Time 0.28 sec\n",
      "Epoch 46 Batch 100 Train Loss 0.1646 Time 28.46 sec\n",
      "Epoch 46 Batch 200 Train Loss 0.1247 Time 31.75 sec\n",
      "Epoch 46 Batch 300 Train Loss 0.1176 Time 34.91 sec\n",
      "Epoch 46 Batch 400 Train Loss 0.1229 Time 38.65 sec\n",
      "Epoch 46 Batch 500 Train Loss 0.1108 Time 41.88 sec\n",
      "Epoch 46 Batch 600 Train Loss 0.1334 Time 45.45 sec\n",
      "Epoch 46 Batch 700 Train Loss 0.1445 Time 48.93 sec\n",
      "Epoch 46 Batch 800 Train Loss 0.1408 Time 52.25 sec\n",
      "Epoch 46 Batch 900 Train Loss 0.1311 Time 55.51 sec\n",
      "Epoch 46 Batch 1000 Train Loss 0.1432 Time 58.71 sec\n",
      "Epoch 46 Batch 1100 Train Loss 0.1408 Time 62.05 sec\n",
      "Epoch 46 Batch 1200 Train Loss 0.1507 Time 65.54 sec\n",
      "Epoch 46 Batch 1300 Train Loss 0.1935 Time 69.21 sec\n",
      "Epoch 46 Batch 1400 Train Loss 0.1631 Time 73.49 sec\n",
      "\n",
      "**Epoch 46 Test Loss 0.1470 \n",
      "\n",
      "**Epoch 46 Train Loss 0.1440\n",
      "Time taken for 1 epoch 824.62 sec\n",
      "\n",
      "Epoch 47 Batch 0 Train Loss 0.1638 Time 0.25 sec\n",
      "Epoch 47 Batch 100 Train Loss 0.1681 Time 28.45 sec\n",
      "Epoch 47 Batch 200 Train Loss 0.0908 Time 31.78 sec\n",
      "Epoch 47 Batch 300 Train Loss 0.1283 Time 35.07 sec\n",
      "Epoch 47 Batch 400 Train Loss 0.1344 Time 38.30 sec\n",
      "Epoch 47 Batch 500 Train Loss 0.1779 Time 41.33 sec\n",
      "Epoch 47 Batch 600 Train Loss 0.1172 Time 45.37 sec\n",
      "Epoch 47 Batch 700 Train Loss 0.1464 Time 49.03 sec\n",
      "Epoch 47 Batch 800 Train Loss 0.1397 Time 51.90 sec\n",
      "Epoch 47 Batch 900 Train Loss 0.1502 Time 55.60 sec\n",
      "Epoch 47 Batch 1000 Train Loss 0.1174 Time 59.16 sec\n",
      "Epoch 47 Batch 1100 Train Loss 0.1548 Time 62.08 sec\n",
      "Epoch 47 Batch 1200 Train Loss 0.1988 Time 66.03 sec\n",
      "Epoch 47 Batch 1300 Train Loss 0.1853 Time 70.01 sec\n",
      "Epoch 47 Batch 1400 Train Loss 0.1596 Time 73.44 sec\n",
      "\n",
      "**Epoch 47 Test Loss 0.1503 \n",
      "\n",
      "**Epoch 47 Train Loss 0.1456\n",
      "Time taken for 1 epoch 825.82 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 0 Train Loss 0.1377 Time 0.28 sec\n",
      "Epoch 48 Batch 100 Train Loss 0.1535 Time 28.60 sec\n",
      "Epoch 48 Batch 200 Train Loss 0.1419 Time 31.86 sec\n",
      "Epoch 48 Batch 300 Train Loss 0.1372 Time 35.23 sec\n",
      "Epoch 48 Batch 400 Train Loss 0.1547 Time 38.42 sec\n",
      "Epoch 48 Batch 500 Train Loss 0.1182 Time 41.57 sec\n",
      "Epoch 48 Batch 600 Train Loss 0.1186 Time 45.47 sec\n",
      "Epoch 48 Batch 700 Train Loss 0.1407 Time 48.71 sec\n",
      "Epoch 48 Batch 800 Train Loss 0.1302 Time 52.11 sec\n",
      "Epoch 48 Batch 900 Train Loss 0.1562 Time 55.87 sec\n",
      "Epoch 48 Batch 1000 Train Loss 0.1370 Time 58.71 sec\n",
      "Epoch 48 Batch 1100 Train Loss 0.1151 Time 62.40 sec\n",
      "Epoch 48 Batch 1200 Train Loss 0.1562 Time 65.99 sec\n",
      "Epoch 48 Batch 1300 Train Loss 0.1545 Time 69.48 sec\n",
      "Epoch 48 Batch 1400 Train Loss 0.1614 Time 73.32 sec\n",
      "\n",
      "**Epoch 48 Test Loss 0.1361 \n",
      "\n",
      "**Epoch 48 Train Loss 0.1383\n",
      "Time taken for 1 epoch 825.75 sec\n",
      "\n",
      "Epoch 49 Batch 0 Train Loss 0.0901 Time 0.26 sec\n",
      "Epoch 49 Batch 100 Train Loss 0.1648 Time 28.71 sec\n",
      "Epoch 49 Batch 200 Train Loss 0.1161 Time 32.02 sec\n",
      "Epoch 49 Batch 300 Train Loss 0.1372 Time 35.02 sec\n",
      "Epoch 49 Batch 400 Train Loss 0.1747 Time 38.65 sec\n",
      "Epoch 49 Batch 500 Train Loss 0.1015 Time 41.95 sec\n",
      "Epoch 49 Batch 600 Train Loss 0.1097 Time 45.17 sec\n",
      "Epoch 49 Batch 700 Train Loss 0.1807 Time 48.88 sec\n",
      "Epoch 49 Batch 800 Train Loss 0.1254 Time 52.79 sec\n",
      "Epoch 49 Batch 900 Train Loss 0.1255 Time 55.97 sec\n",
      "Epoch 49 Batch 1000 Train Loss 0.1354 Time 59.30 sec\n",
      "Epoch 49 Batch 1100 Train Loss 0.1412 Time 62.25 sec\n",
      "Epoch 49 Batch 1200 Train Loss 0.1350 Time 66.27 sec\n",
      "Epoch 49 Batch 1300 Train Loss 0.1515 Time 69.56 sec\n",
      "Epoch 49 Batch 1400 Train Loss 0.1377 Time 73.88 sec\n",
      "\n",
      "**Epoch 49 Test Loss 0.1305 \n",
      "\n",
      "**Epoch 49 Train Loss 0.1312\n",
      "Time taken for 1 epoch 828.93 sec\n",
      "\n",
      "Epoch 50 Batch 0 Train Loss 0.1024 Time 0.26 sec\n",
      "Epoch 50 Batch 100 Train Loss 0.1347 Time 28.71 sec\n",
      "Epoch 50 Batch 200 Train Loss 0.1441 Time 31.78 sec\n",
      "Epoch 50 Batch 300 Train Loss 0.1217 Time 35.17 sec\n",
      "Epoch 50 Batch 400 Train Loss 0.1151 Time 38.61 sec\n",
      "Epoch 50 Batch 500 Train Loss 0.1351 Time 44.03 sec\n",
      "Epoch 50 Batch 600 Train Loss 0.1060 Time 45.54 sec\n",
      "Epoch 50 Batch 700 Train Loss 0.1270 Time 49.34 sec\n",
      "Epoch 50 Batch 800 Train Loss 0.0895 Time 52.84 sec\n",
      "Epoch 50 Batch 900 Train Loss 0.1287 Time 55.66 sec\n",
      "Epoch 50 Batch 1000 Train Loss 0.1123 Time 58.79 sec\n",
      "Epoch 50 Batch 1100 Train Loss 0.1108 Time 62.42 sec\n",
      "Epoch 50 Batch 1200 Train Loss 0.1502 Time 66.29 sec\n",
      "Epoch 50 Batch 1300 Train Loss 0.2157 Time 70.04 sec\n",
      "Epoch 50 Batch 1400 Train Loss 0.1688 Time 73.62 sec\n",
      "\n",
      "**Epoch 50 Test Loss 0.1552 \n",
      "\n",
      "**Epoch 50 Train Loss 0.1408\n",
      "Time taken for 1 epoch 831.25 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_epoch = time.time()\n",
    "    start_batch = time.time()\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    total_test_loss = 0\n",
    "    \n",
    "    total_batch = len(sorted_train)//batch_size -1 \n",
    "    total_test_batch = len(sorted_test)//batch_size -1 \n",
    "    \n",
    "    \n",
    "    # Train Loss\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(get_batches(sorted_train, batch_size, threshold)):\n",
    "    \n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            end_batch = time.time()\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Train Loss {batch_loss.numpy():.4f} Time {end_batch - start_batch:.2f} sec') \n",
    "            start_batch = time.time()\n",
    "                    \n",
    "        # Test Loss\n",
    "        \n",
    "        if(batch == total_batch):\n",
    "        \n",
    "            for (test_batch, (test_inp, test_targ)) in enumerate(get_batches(sorted_test, batch_size, threshold)):\n",
    "\n",
    "                test_batch_loss = train_step(test_inp, test_targ, enc_hidden)\n",
    "                total_test_loss += test_batch_loss \n",
    "                \n",
    "            print(f'\\n**Epoch {epoch+1} Test Loss {total_test_loss/total_test_batch:.4f} \\n')\n",
    "    \n",
    "    \n",
    "    end_epoch = time.time()\n",
    "    print(f'**Epoch {epoch+1} Train Loss {total_loss/total_batch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {end_epoch -start_epoch:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "#checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    \n",
    "    sentence = sentence_to_integer(sentence)\n",
    "    \n",
    "    #print(sentence)\n",
    "    \n",
    "    sentence.insert(0,voc2int['<GO>'])\n",
    "    sentence.append(voc2int['<EOS>'])\n",
    "    \n",
    "    #print(sentence)\n",
    "    \n",
    "    inputs = tf.convert_to_tensor([sentence])\n",
    "    \n",
    "    #print(inputs)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([voc2int['<GO>']], 0)\n",
    "\n",
    "    for t in range(max_length):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input,dec_hidden,enc_out)\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "    \n",
    "        if int2voc[predicted_id] == '<EOS>':\n",
    "            return result, sentence\n",
    "        else:\n",
    "            result.append(predicted_id)\n",
    "            \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    \n",
    "    #print('Input:', sentence)\n",
    "    \n",
    "    print('Predicted translation:', translate_sentence(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: Bugün hava çok sıcak\n"
     ]
    }
   ],
   "source": [
    "translate(\"Bugünn hva çk sıcak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: Hayvanları sevdi\n"
     ]
    }
   ],
   "source": [
    "translate(\"Hyvanları sedi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: Size teşekkür ederim\n"
     ]
    }
   ],
   "source": [
    "translate(\"Size teşekkr edröm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: Alışveriş yapmaya gitti\n"
     ]
    }
   ],
   "source": [
    "translate(\"AAlışveriş ypmaya gidti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: gazete\n"
     ]
    }
   ],
   "source": [
    "translate(\"gazte\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
